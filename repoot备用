# ==============================================================================
#                 äººæ‰åˆ†ææŠ¥å‘ŠAI (Talent Report AI) v4.1
#                    (Final Custom Fit Edition)
# ==============================================================================
# ç‰ˆæœ¬è¯´æ˜:
# - ã€æœ€ç»ˆå®šåˆ¶ã€‘ä»£ç ç°åœ¨100%ç²¾ç¡®åŒ¹é…ä½ æˆªå›¾ä¸­çš„Notionæ•°æ®åº“åˆ—åã€‚
# - ã€ç²¾å‡†è¯»å–ã€‘åªè¯»å–`å€™é€‰äººå§“å`, `åŒ¹é…åº¦è¯„åˆ†`, `è¯„åˆ†ç†ç”±`, `æ‹›è˜çŠ¶æ€`, `é¢è¯•å®˜åé¦ˆ`è¿™å‡ åˆ—ã€‚
# - ã€ç¨³å®šå¯é ã€‘é›†æˆäº†ä¹‹å‰æ‰€æœ‰ç‰ˆæœ¬çš„bugä¿®å¤å’Œä¼˜åŒ–ã€‚
# - ã€æ ¸å¿ƒäº§å‡ºã€‘ç”Ÿæˆä¸€ä»½åŒ…å«æ·±åº¦æ–‡å­—æ´å¯Ÿå’Œæ¸…æ™°æ•°æ®è¡¨æ ¼çš„Word(.docx)æŠ¥å‘Šã€‚
# ==============================================================================

import os
import sys
import re
import pandas as pd
from docx import Document
from docx.shared import Inches, Pt
from docx.oxml.ns import qn
from datetime import datetime
import google.generativeai as genai
from notion_client import Client, APIResponseError
from dotenv import load_dotenv
import time
import PyPDF2

# --- 0. å‡†å¤‡å·¥ä½œ ---
def setup_environment():
    print("--- å¯åŠ¨å‡†å¤‡å·¥ä½œ (v4.1 æœ€ç»ˆå®šåˆ¶ç‰ˆ) ---")
    os.makedirs('output', exist_ok=True)
    os.makedirs('resumes', exist_ok=True)
    print("  - è¾“å‡ºä¸ç®€å†ç›®å½•å·²ç¡®è®¤ã€‚")

# --- 1. WordæŠ¥å‘Šç”Ÿæˆå™¨ ---
def generate_word_report(narratives, stats, filename):
    print("âœï¸ æ­£åœ¨ç»„è£…Word(.docx)æŠ¥å‘Š...")
    try:
        doc = Document()
        doc.styles['Normal'].font.name = u'å®‹ä½“'
        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'å®‹ä½“')
        
        doc.add_heading(narratives['title'], level=0)
        p = doc.add_paragraph(); p.add_run(narratives['subtitle']).italic = True
        p = doc.add_paragraph(); p.add_run(f"æŠ¥å‘Šç”Ÿæˆæ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}").font.size = Pt(9)
        doc.add_page_break()

        def add_df_to_doc(df, title=""):
            if title: doc.add_paragraph(title, style='Heading 3')
            if df.empty:
                doc.add_paragraph("  (æ— ç›¸å…³æ•°æ®å¯ç”Ÿæˆæ­¤è¡¨æ ¼)")
                doc.add_paragraph()
                return
            table = doc.add_table(rows=1, cols=len(df.columns)); table.style = 'Table Grid'
            hdr_cells = table.rows[0].cells
            for i, col_name in enumerate(df.columns): hdr_cells[i].text = str(col_name)
            for _, row in df.iterrows():
                row_cells = table.add_row().cells
                for i, value in enumerate(row): row_cells[i].text = str(value)
            doc.add_paragraph()

        for chapter in narratives['chapters']:
            doc.add_heading(chapter['title'], level=1)
            p = doc.add_paragraph(str(chapter['content'])); p.style = doc.styles['Normal']
            
            if chapter['title'] == 'å†…éƒ¨æ•°æ®æ·±åº¦æ´å¯Ÿ':
                df_status = pd.DataFrame(list(stats.get('status_distribution', {}).items()), columns=['æ‹›è˜çŠ¶æ€', 'å€™é€‰äººæ•°é‡'])
                add_df_to_doc(df_status, title="è¡¨1: å€™é€‰äººæ‹›è˜çŠ¶æ€åˆ†å¸ƒ")
                
                df_skills = pd.DataFrame(list(stats.get('feedback_skills_distribution', {}).items()), columns=['æŠ€èƒ½', 'æåŠæ¬¡æ•°'])
                add_df_to_doc(df_skills, title="è¡¨2: ç»¼åˆé«˜é¢‘æŠ€èƒ½è¯")
            
            doc.add_paragraph()

        doc.save(filename)
        print(f"  - WordæŠ¥å‘Šå·²æˆåŠŸä¿å­˜ï¼")
    except Exception as e:
        print(f"âŒ ç”ŸæˆWordæŠ¥å‘Šæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

# --- 2. åˆå§‹åŒ–ä¸é…ç½®åŠ è½½ ---
def initialize():
    print("\n" + "="*70); print("ğŸš€ å¯åŠ¨äººæ‰åˆ†ææŠ¥å‘ŠAIå¼•æ“ (v4.1)...")
    load_dotenv(); config = {"NOTION_TOKEN": os.getenv("NOTION_API_KEY"), "API_KEY": os.getenv("GEMINI_API_KEY"), "CANDIDATE_DB_ID": os.getenv("LLM_CANDIDATE_DB_ID")}
    if not all(config.values()):
        print("âŒ è‡´å‘½é”™è¯¯ï¼šå…³é”®é…ç½®ç¼ºå¤±ï¼"); sys.exit(1)
    try:
        notion = Client(auth=config["NOTION_TOKEN"]); genai.configure(api_key=config["API_KEY"])
        gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest'); print("âœ… åˆå§‹åŒ–æˆåŠŸï¼"); return notion, gemini_model, config
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"); sys.exit(1)

# --- 3. æ•°æ®è¯»å–æ¨¡å— ---
def read_resumes_from_folder(folder_path):
    absolute_folder_path = os.path.abspath(folder_path)
    print(f"ğŸ“‚ æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„è¯»å–ç®€å†: '{absolute_folder_path}'")
    if not os.path.isdir(absolute_folder_path):
        print(f"  - âŒ é”™è¯¯: ç®€å†æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨ï¼"); return ""
    files_in_folder = os.listdir(absolute_folder_path)
    if not files_in_folder:
        print("  - ğŸŸ¡ è­¦å‘Š: ç®€å†æ–‡ä»¶å¤¹æ˜¯ç©ºçš„ã€‚"); return ""
    resume_contents = []
    for filename in files_in_folder:
        file_path = os.path.join(absolute_folder_path, filename)
        content = f"\n--- ç®€å†æ¥æº: {filename} ---\n"
        try:
            if filename.lower().endswith('.pdf'):
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    if reader.is_encrypted: print(f"  - è·³è¿‡åŠ å¯†çš„PDF: {filename}"); continue
                    for page in reader.pages: content += page.extract_text() or ""
            elif filename.lower().endswith('.docx'):
                doc = Document(file_path);
                for para in doc.paragraphs: content += para.text + '\n'
            elif filename.lower().endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: content += f.read()
            else: continue
            resume_contents.append(content)
        except Exception as e: print(f"  - âŒ è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
    print(f"  - æˆåŠŸè¯»å– {len(resume_contents)} ä»½ç®€å†ã€‚")
    return "\n".join(resume_contents)

def fetch_notion_data(notion, db_id):
    print("\n" + "="*70); print("â³ æ­£åœ¨ä»Notionæ‹‰å–æ ¸å¿ƒæ•°æ®...")
    all_pages, has_more, start_cursor = [], True, None
    while has_more:
        try:
            response = notion.databases.query(database_id=db_id, start_cursor=start_cursor, page_size=100)
            all_pages.extend(response.get("results", [])); has_more = response.get("has_more", False); start_cursor = response.get("next_cursor")
        except APIResponseError as e:
            if "Could not find database" in str(e): print(f"âŒ æŸ¥è¯¢Notionå‡ºé”™: æ‰¾ä¸åˆ°æ•°æ®åº“ã€‚è¯·æ£€æŸ¥ .env çš„ LLM_CANDIDATE_DB_IDã€‚")
            elif "is not shared with the integration" in str(e): print(f"âŒ æŸ¥è¯¢Notionå‡ºé”™: æ•°æ®åº“æœªåˆ†äº«ç»™æœºå™¨äººã€‚è¯·ä¸ºIDä¸º'{db_id}'çš„æ•°æ®åº“ã€æ·»åŠ è¿æ¥ã€‘ã€‚")
            else: print(f"âŒ æŸ¥è¯¢Notionæ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
    if not all_pages: print("ğŸŸ¡ æœªå‘ç°Notionå€™é€‰äººæ•°æ®ã€‚è¯·ç¡®è®¤æ•°æ®åº“IDæ­£ç¡®ä¸”å·²åˆ†äº«ç»™æœºå™¨äººã€‚"); return pd.DataFrame()
    
    all_candidates = []
    for page in all_pages:
        props = page.get("properties", {})
        # ã€æœ€ç»ˆå®šåˆ¶ã€‘å®Œå…¨åŒ¹é…ä½ æˆªå›¾ä¸­çš„åˆ—å
        def get_prop(prop_name, prop_type):
            data = props.get(prop_name)
            if not data: return None
            if prop_type == 'title': return data.get('title', [{}])[0].get('plain_text')
            if prop_type == 'rich_text': return ''.join(p.get('plain_text', '') for p in data.get('rich_text', []))
            if prop_type == 'multi_select': return [s['name'] for s in data.get('multi_select', [])]
            if prop_type == 'number': return data.get('number')
            return None
            
        all_candidates.append({
            'name': get_prop("å€™é€‰äººå§“å", "title"),
            'status': get_prop("æ‹›è˜çŠ¶æ€", "multi_select"),
            'feedback': get_prop("é¢è¯•å®˜åé¦ˆ", "rich_text"),
            'rating': get_prop("åŒ¹é…åº¦è¯„åˆ†", "number"),
            'reason': get_prop("è¯„åˆ†ç†ç”±", "rich_text")
        })
    df = pd.DataFrame(all_candidates).dropna(subset=['name'])
    print(f"âœ… Notionæ•°æ®æ‹‰å–å®Œæˆï¼å…±å¤„ç† {len(df)} æ¡è®°å½•ã€‚"); return df

# --- 4. AIåˆ†ææ¨¡å— ---
def ask_gemini(gemini_model, prompt, retries=2):
    for i in range(retries):
        try:
            response = gemini_model.generate_content(prompt, request_options={"timeout": 180})
            return response.text
        except Exception as e: print(f"  - è­¦å‘Š: è°ƒç”¨AIå¤±è´¥: {e}. é‡è¯•..."); time.sleep(2)
    return "AI æœªèƒ½ç”Ÿæˆå†…å®¹ã€‚"

def generate_ai_narrative(gemini_model, jd_text, stats, resume_summary):
    print("\n" + "="*70); print("ğŸ§  æ­£åœ¨è°ƒç”¨Geminiè¿›è¡ŒåŒæºæ•°æ®æ·±åº¦åˆ†æ...")
    # ã€æ›´æ–°ã€‘æ•°æ®æ‘˜è¦ç°åœ¨åŒ…å«æ›´ä¸°å¯Œçš„ä¿¡æ¯
    data_summary = f"""
    å†…éƒ¨æ•°æ®æ‘˜è¦:
    - Notionæ•°æ®åº“å€™é€‰äººæ€»æ•°: {stats['total_candidates']}
    - æ‹›è˜çŠ¶æ€åˆ†å¸ƒ: {stats.get('status_distribution', 'N/A')}
    - ç»¼åˆé«˜é¢‘æŠ€èƒ½è¯ (æ¥è‡ªè¯„åˆ†ç†ç”±ã€é¢è¯•åé¦ˆå’Œç®€å†): {stats.get('feedback_skills_distribution', 'N/A')}
    - æœ¬åœ°ç®€å†æ–‡ä»¶å¤¹æ‘˜è¦ (ä»…æ˜¾ç¤ºéƒ¨åˆ†ç”¨äºä¸Šä¸‹æ–‡): {resume_summary[:3000]}...
    """
    chapters = [
        {"title": "æ‰§è¡Œæ‘˜è¦", "prompt": f"ä½œä¸ºAIäººæ‰æˆ˜ç•¥é¡¾é—®ï¼Œè¯·ç»“åˆä½ å¯¹å½“å‰å¤§æ¨¡å‹äººæ‰å¸‚åœºçš„ç†è§£ï¼Œå¹¶åŸºäºä»¥ä¸‹ã€Notionæ•°æ®åº“ã€‘å’Œã€æœ¬åœ°ç®€å†æ–‡ä»¶å¤¹ã€‘çš„ç»¼åˆæ•°æ®æ‘˜è¦ï¼Œæ’°å†™ä¸€ä»½è¯¦å°½çš„æ‰§è¡Œæ‘˜è¦ï¼Œç¯‡å¹…ä¸å°‘äº300å­—ã€‚\n\n{data_summary}"},
        {"title": "äººæ‰å¸‚åœºå®è§‚åˆ†æ", "prompt": f"ä½œä¸ºè¡Œä¸šåˆ†æå¸ˆï¼Œè¯·å•çº¯åŸºäºä½ åºå¤§çš„çŸ¥è¯†åº“ï¼Œåˆ†æå½“å‰å¸‚åœºä¸Šé€šç”¨AIäººæ‰ï¼Œç‰¹åˆ«æ˜¯å¤§æ¨¡å‹ç®—æ³•å·¥ç¨‹å¸ˆâ€œè´µåœ¨å“ªé‡Œâ€ã€‚è¯·ä»æŠ€æœ¯å£å’ã€äººæ‰ä¾›éœ€ã€è¡Œä¸šåº”ç”¨ä»·å€¼ç­‰è§’åº¦å±•å¼€ï¼Œå†™ä¸€æ®µè‡³å°‘800å­—çš„æ·±åº¦åˆ†æã€‚"},
        {"title": "å†…éƒ¨æ•°æ®æ·±åº¦æ´å¯Ÿ", "prompt": f"ç°åœ¨ï¼Œè¯·èšç„¦äºæˆ‘æä¾›çš„å†…éƒ¨ç»¼åˆæ•°æ®ã€‚æ·±å…¥è§£è¯»è¿™ä»½æ•°æ®æ‘˜è¦ï¼Œç‰¹åˆ«æ˜¯ä»é¢è¯•åé¦ˆå’Œè¯„åˆ†ç†ç”±ä¸­ä½“ç°çš„æŠ€èƒ½ç‚¹ï¼Œä»¥åŠæ‹›è˜æ¼æ–—ï¼ˆçŠ¶æ€åˆ†å¸ƒï¼‰ï¼Œè¯¦ç»†åˆ†ææˆ‘ä»¬å¸å¼•çš„äººæ‰è´¨é‡å¦‚ä½•ï¼Ÿ\n\n{data_summary}"},
        {"title": "æˆ˜ç•¥è¡ŒåŠ¨å»ºè®®", "prompt": f"ç»¼åˆä»¥ä¸Šæ‰€æœ‰åˆ†æï¼Œè¯·ä¸ºæˆ‘ä»¬çš„æ‹›è˜å›¢é˜Ÿæå‡º5æ¡å…·ä½“çš„ã€å¯ç«‹å³æ‰§è¡Œçš„ã€è¯¦å°½çš„æˆ˜ç•¥å»ºè®®ï¼Œæ¯æ¡å»ºè®®éƒ½éœ€é˜è¿°å…¶èƒŒåçš„é€»è¾‘å’Œé¢„æœŸæ•ˆæœã€‚\n\n{data_summary}"},
    ]
    narratives = {"title": "AIå¤§æ¨¡å‹å·¥ç¨‹å¸ˆäººæ‰åˆ†ææŠ¥å‘Š", "subtitle": "åŸºäº[Notionæ•°æ®åº“+æœ¬åœ°ç®€å†]çš„åŒæºæ·±åº¦æ´å¯Ÿ", "chapters": []}
    for chapter in chapters:
        print(f"  - æ­£åœ¨ç”Ÿæˆç« èŠ‚: [{chapter['title']}]...")
        content = ask_gemini(gemini_model, chapter['prompt'])
        narratives['chapters'].append({"title": chapter['title'], "content": content})
    print("âœ… æ‰€æœ‰æŠ¥å‘Šç« èŠ‚å·²ç”±AIç”Ÿæˆï¼")
    return narratives

# --- 5. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    setup_environment()
    notion, gemini_model, config = initialize()
    
    resume_folder_path = 'resumes' 

    df_notion_data = fetch_notion_data(notion, config["CANDIDATE_DB_ID"])
    resumes_text = read_resumes_from_folder(resume_folder_path)

    if df_notion_data.empty and not resumes_text:
        print("\nâŒ è‡´å‘½é”™è¯¯: Notionå’Œæœ¬åœ°æ–‡ä»¶å¤¹å‡æ— æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
    else:
        print("\n" + "="*70); print("ğŸ”„ æ­£åœ¨è¿›è¡Œæœ€ç»ˆçš„é‡åŒ–åˆ†æ...")
        # ã€æ›´æ–°ã€‘ä»æ›´å¤šåˆ—æå–æ–‡æœ¬è¿›è¡Œåˆ†æ
        all_text_for_analysis = ' '.join(df_notion_data['feedback'].dropna().astype(str)) + \
                                ' '.join(df_notion_data['reason'].dropna().astype(str)) + \
                                " " + resumes_text
        
        stats = {}
        stats['total_candidates'] = len(df_notion_data)
        
        if 'status' in df_notion_data.columns and not df_notion_data['status'].dropna().empty:
            status_flat = [s for sublist in df_notion_data['status'].dropna() for s in sublist]
            stats['status_distribution'] = pd.Series(status_flat).value_counts().to_dict()
        else:
            stats['status_distribution'] = {}
            
        skills_to_track = ['Python', 'PyTorch', 'TensorFlow', 'Transformer', 'DeepSpeed', 'RLHF', 'CUDA', 'æ˜‡è…¾', 'é‡‘è', 'å¾®è°ƒ', 'é‡åŒ–', 'å·¥ç¨‹åŒ–', 'æ¶æ„', 'NLP']
        skill_counts = {skill: all_text_for_analysis.lower().count(skill.lower()) for skill in skills_to_track}
        stats['feedback_skills_distribution'] = {k: v for k, v in sorted(skill_counts.items(), key=lambda item: item[1], reverse=True) if v > 0}
        print("âœ… é‡åŒ–åˆ†æå®Œæˆï¼")

        try:
            with open('jd_template.txt', 'r', encoding='utf-8') as f:
                jd_text = f.read()
        except FileNotFoundError:
            print("âŒ é”™è¯¯: 'jd_template.txt' æ–‡ä»¶æœªæ‰¾åˆ°ã€‚"); sys.exit(1)
            
        narratives = generate_ai_narrative(gemini_model, jd_text, stats, resumes_text)
        
        if narratives:
            base_filename = f"output/Talent_Analysis_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            docx_filename = f"{base_filename}.docx"
            generate_word_report(narratives, stats, docx_filename)
            
            print(f"\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
            print(f"   - Wordç‰ˆ (åŒ…å«æ–‡å­—ä¸è¡¨æ ¼): {docx_filename}")

    print("\n" + "="*70); print("æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•...")