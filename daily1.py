# ==============================================================================
#                      å‘¨æœŸæ€§å¤ç›˜AI (Periodic Review AI) v5.1
#                      (åˆ†é¡µæŸ¥è¯¢ä¿®å¤ç‰ˆ)
# ==============================================================================
# ç‰ˆæœ¬è¯´æ˜ v5.1:
# - ã€æ ¸å¿ƒä¿®å¤-åˆ†é¡µæŸ¥è¯¢ã€‘: å‡çº§äº† fetch_data_for_period å‡½æ•°ï¼Œç°åœ¨èƒ½å¤Ÿé€šè¿‡å¾ªç¯
#                       åˆ†é¡µï¼Œå®Œæ•´æ‹‰å–è¶…è¿‡100æ¡çš„å‘¨æœŸå†…æ•°æ®ï¼Œè§£å†³äº†æ•°æ®æ‹‰å–
#                       ä¸å®Œæ•´çš„é—®é¢˜ã€‚
# - ã€ä¿ç•™ç²¾åã€‘: v5.0çš„æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒ…æ‹¬ChromaDBè®°å¿†ã€è¡¥å†™æ—¥æŠ¥ç­‰å‡å®Œæ•´ä¿ç•™ã€‚
# ==============================================================================

import os
import google.generativeai as genai
from notion_client import Client, APIResponseError
from dotenv import load_dotenv
from datetime import datetime, timezone, timedelta
import sys
import re
import threading
import argparse
import chromadb

def print_separator():
    print("\n" + "="*70 + "\n")

# --- æ¨¡å—ï¼šæ–‡æœ¬å‡€åŒ–å™¨ä¸è®­ç»ƒä¸­å¿ƒå†™å…¥ (v4.0åŸæ ·ä¿ç•™) ---
def clean_text(text):
    if not isinstance(text, str): return ""
    return re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)

def write_to_training_hub(notion, task_type, input_text, output_text, source_db_name, source_page_id, config):
    training_hub_db_id = config.get("TRAINING_HUB_DB_ID")
    if not training_hub_db_id:
        print(">> [è®­ç»ƒä¸­å¿ƒ] æœªé…ç½®æ•°æ®åº“ID (TRAINING_HUB_DATABASE_ID)ï¼Œè·³è¿‡å†™å…¥ã€‚")
        return
    try:
        safe_input_text = clean_text(str(input_text))[:1990]
        safe_output_text = clean_text(str(output_text))[:1990]
        training_title = f"ã€{task_type}ã€‘{datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d')} å‘¨æœŸæ€§æŠ¥å‘Šæ‘˜è¦"
        properties_data = {
            "è®­ç»ƒä»»åŠ¡": {"title": [{"text": {"content": training_title}}]},
            "ä»»åŠ¡ç±»å‹": {"select": {"name": task_type}},
            "æºæ•°æ® (Input)": {"rich_text": [{"text": {"content": safe_input_text}}]},
            "ç†æƒ³è¾“å‡º (Output)": {"rich_text": [{"text": {"content": safe_output_text}}]},
            "æ ‡æ³¨çŠ¶æ€": {"select": {"name": "å¾…å®¡æ ¸"}}
        }
        relation_column_map = {'DailyReview': config.get("RELATION_LINK_REVIEW_NAME", "æºé“¾æ¥-æ¯æ—¥å¤ç›˜")}
        if source_db_name in relation_column_map and source_page_id:
            column_to_update = relation_column_map[source_db_name]
            properties_data[column_to_update] = {"relation": [{"id": source_page_id}]}
        notion.pages.create(parent={"database_id": training_hub_db_id}, properties=properties_data)
        print(f">> [è®­ç»ƒä¸­å¿ƒ] å·²æˆåŠŸè®°å½•ä¸€æ¡ '{task_type}' è®­ç»ƒæ•°æ®ã€‚")
    except Exception as e:
        print(f"!! [è®­ç»ƒä¸­å¿ƒ] å†™å…¥æ—¶å‡ºé”™: {e}")

# --- å‘é‡æ•°æ®åº“æ ¸å¿ƒæ¨¡å— (v5.0åŸæ ·ä¿ç•™) ---
class VectorMemory:
    def __init__(self, path, collection_name):
        self.client = None
        self.collection = None
        if not path or not collection_name:
            print("ğŸŸ¡ [è®°å¿†æ¨¡å—] æœªé…ç½®CHROMA_DB_PATHæˆ–CHROMA_COLLECTION_NAMEï¼Œå°†è·³è¿‡æ‰€æœ‰å‘é‡æ“ä½œã€‚")
            return
        try:
            print(f"ğŸ§  [è®°å¿†æ¨¡å—] æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
            self.client = chromadb.PersistentClient(path=path)
            self.collection = self.client.get_or_create_collection(name=collection_name, metadata={"hnsw:space": "cosine"})
            print(f"  - æˆåŠŸè¿æ¥åˆ°é›†åˆ '{collection_name}'ã€‚")
        except Exception as e:
            print(f"âŒ [è®°å¿†æ¨¡å—] åˆå§‹åŒ–ChromaDBå¤±è´¥: {e}")
            self.client = None
    def add_memory(self, report_text, metadata):
        if not self.collection: return
        try:
            doc_id = f"{metadata['type']}-{metadata['date']}"
            print(f"  - æ­£åœ¨å°†æŠ¥å‘Š '{doc_id}' å‘é‡åŒ–å¹¶å­˜å…¥é•¿æœŸè®°å¿†...")
            embedding_model = 'models/embedding-001'
            embedding = genai.embed_content(model=embedding_model, content=report_text, task_type="RETRIEVAL_DOCUMENT")["embedding"]
            self.collection.add(embeddings=[embedding], documents=[report_text], metadatas=[metadata], ids=[doc_id])
            print(f"  - âœ… è®°å¿†èƒ¶å›Š '{doc_id}' å·²æˆåŠŸå­˜å…¥å‘é‡æ•°æ®åº“ï¼")
        except Exception as e:
            print(f"  - âŒ [è®°å¿†æ¨¡å—] å­˜å…¥è®°å¿†æ—¶å‡ºé”™: {e}")
    def retrieve_memory(self, query_text, n_results=3):
        if not self.collection: return ""
        try:
            print(f"  - ğŸ§  æ­£åœ¨åŸºäºé—®é¢˜ '{query_text[:30]}...' æ£€ç´¢ç›¸å…³å†å²è®°å¿†...")
            embedding_model = 'models/embedding-001'
            query_embedding = genai.embed_content(model=embedding_model, content=query_text, task_type="RETRIEVAL_QUERY")["embedding"]
            results = self.collection.query(query_embeddings=[query_embedding], n_results=n_results)
            if not results or not results['documents'] or not results['documents'][0]:
                print("  - æœªæ‰¾åˆ°ç›¸å…³å†å²è®°å¿†ã€‚")
                return ""
            retrieved_docs = []
            for i, doc in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i]
                entry = f"--- å†å²æ´å¯Ÿ ({meta.get('date', 'æœªçŸ¥æ—¥æœŸ')} {meta.get('type', 'æŠ¥å‘Š')}):\n{doc}\n---"
                retrieved_docs.append(entry)
            print(f"  - âœ… æˆåŠŸæ£€ç´¢åˆ° {len(retrieved_docs)} æ¡ç›¸å…³å†å²è®°å¿†ã€‚")
            return "\n".join(retrieved_docs)
        except Exception as e:
            print(f"  - âŒ [è®°å¿†æ¨¡å—] æ£€ç´¢è®°å¿†æ—¶å‡ºé”™: {e}")
            return ""

# --- åˆå§‹åŒ–ä¸é…ç½®åŠ è½½ (v5.0åŸæ ·ä¿ç•™) ---
def initialize():
    print_separator()
    print("ğŸš€ å¯åŠ¨å‘¨æœŸæ€§å¤ç›˜AIå¼•æ“ (v5.1)...")
    load_dotenv()
    config = {
        "NOTION_TOKEN": os.getenv("NOTION_API_KEY"), "API_KEY": os.getenv("GEMINI_API_KEY"),
        "LOG_DB_ID": os.getenv("TOOLBOX_LOG_DATABASE_ID"), "BRAIN_DB_ID": os.getenv("CORE_BRAIN_DATABASE_ID"),
        "CANDIDATE_DB_ID": os.getenv("CANDIDATE_DATABASE_ID"), "REVIEW_DB_ID": os.getenv("DAILY_REVIEW_DATABASE_ID"),
        "TRAINING_HUB_DB_ID": os.getenv("TRAINING_HUB_DATABASE_ID"), "RELATION_LINK_REVIEW_NAME": os.getenv("RELATION_LINK_REVIEW_NAME", "æºé“¾æ¥-æ¯æ—¥å¤ç›˜"),
        "CHROMA_DB_PATH": os.getenv("CHROMA_DB_PATH"), "CHROMA_COLLECTION_NAME": os.getenv("CHROMA_COLLECTION_NAME")
    }
    if not all([config["NOTION_TOKEN"], config["API_KEY"], config["REVIEW_DB_ID"]]):
        print("âŒ é”™è¯¯ï¼šå…³é”®é…ç½®ç¼ºå¤±ï¼(NOTION_API_KEY, GEMINI_API_KEY, DAILY_REVIEW_DATABASE_ID å¿…é¡»åœ¨ .env æ–‡ä»¶ä¸­é…ç½®)")
        sys.exit(1)
    try:
        notion = Client(auth=config["NOTION_TOKEN"])
        genai.configure(api_key=config["API_KEY"])
        gemini_model = genai.GenerativeModel('models/gemini-2.5-pro')
        memory = VectorMemory(config["CHROMA_DB_PATH"], config["CHROMA_COLLECTION_NAME"])
        print("âœ… Notion, Gemini API å’Œ å‘é‡è®°å¿†æ¨¡å— åˆå§‹åŒ–æˆåŠŸï¼")
        return notion, gemini_model, memory, config
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"); sys.exit(1)

# --- æ—¶é—´èŒƒå›´è®¡ç®— (v5.0åŸæ ·ä¿ç•™) ---
def get_date_range(report_type, specific_date_str=None):
    if specific_date_str:
        try:
            beijing_tz = timezone(timedelta(hours=8))
            target_date = datetime.strptime(specific_date_str, '%Y-%m-%d')
            start_date_local = target_date.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=beijing_tz)
            end_date_local = target_date.replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=beijing_tz)
            return start_date_local.astimezone(timezone.utc), end_date_local.astimezone(timezone.utc)
        except ValueError:
            print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: '{specific_date_str}'ã€‚è¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼ã€‚ç¨‹åºé€€å‡ºã€‚"); sys.exit(1)
            
    end_date = datetime.now(timezone.utc)
    if report_type == 'daily':
        today_beijing = datetime.now(timezone(timedelta(hours=8))).date()
        start_of_day_beijing = datetime.combine(today_beijing, datetime.min.time(), tzinfo=timezone(timedelta(hours=8)))
        start_date = start_of_day_beijing.astimezone(timezone.utc)
    elif report_type == 'weekly': start_date = end_date - timedelta(days=7)
    elif report_type == 'monthly': start_date = end_date - timedelta(days=30)
    elif report_type == 'quarterly': start_date = end_date - timedelta(days=90)
    elif report_type == 'yearly': start_date = end_date - timedelta(days=365)
    else:
        today_beijing = datetime.now(timezone(timedelta(hours=8))).date()
        start_of_day_beijing = datetime.combine(today_beijing, datetime.min.time(), tzinfo=timezone(timedelta(hours=8)))
        start_date = start_of_day_beijing.astimezone(timezone.utc)
    return start_date, end_date

# --- ã€ã€ã€ æ ¸å¿ƒä¿®å¤: æ•°æ®æ‹‰å–å‡½æ•°å‡çº§ä¸ºåˆ†é¡µæŸ¥è¯¢ ã€‘ã€‘ã€‘ ---
def fetch_data_for_period(notion, db_id, db_name, start_date, end_date):
    """æ ¹æ®æŒ‡å®šçš„æ—¶é—´èŒƒå›´ä»Notionæ‹‰å–æ‰€æœ‰æ•°æ®ï¼ˆæ”¯æŒåˆ†é¡µï¼‰ã€‚"""
    if not db_id:
        print(f"ğŸŸ¡ è·³è¿‡ [{db_name}]ï¼šæœªåœ¨.envä¸­é…ç½®å…¶æ•°æ®åº“IDã€‚"); return ""
    print(f"â³ æ­£åœ¨ä» [{db_name}] æ•°æ®åº“æ‹‰å–å‘¨æœŸå†…æ•°æ®...")
    
    try:
        all_pages = []
        start_cursor = None
        
        # æ ¸å¿ƒä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ while å¾ªç¯å¤„ç†åˆ†é¡µ
        while True:
            response = notion.databases.query(
                database_id=db_id, 
                filter={
                    "and": [
                        {"timestamp": "last_edited_time", "last_edited_time": {"on_or_after": start_date.isoformat()}},
                        {"timestamp": "last_edited_time", "last_edited_time": {"before": end_date.isoformat()}}
                    ]
                },
                start_cursor=start_cursor, # ä¼ å…¥ä¸‹ä¸€é¡µçš„èµ·å§‹ä½ç½®
                page_size=100 # æ¯æ¬¡æœ€å¤šæ‹‰100æ¡
            )
            
            pages_on_this_page = response.get("results", [])
            all_pages.extend(pages_on_this_page)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
            if response.get("has_more"):
                start_cursor = response.get("next_cursor") # è·å–ä¸‹ä¸€é¡µçš„ "é—¨ç¥¨"
            else:
                break # æ²¡æœ‰æ›´å¤šé¡µäº†ï¼Œé€€å‡ºå¾ªç¯
        
        if not all_pages:
            print(f"  - åœ¨ [{db_name}] ä¸­æœªå‘ç°è¯¥å‘¨æœŸå†…çš„æ›´æ–°ã€‚"); return ""
        
        content_list = []
        for page in all_pages:
            title, properties = "[æ— æ ‡é¢˜]", page.get("properties", {})
            title_prop_names = ["ä¸»é¢˜", "æ—¥å¿—æ ‡é¢˜ åç§°", "å€™é€‰äººå§“å"]
            for prop_name in title_prop_names:
                if prop_name in properties and properties[prop_name].get("type") == "title":
                    title_parts = properties[prop_name].get("title", [])
                    if title_parts: title = title_parts[0].get("plain_text", "[ç©ºæ ‡é¢˜]"); break
            
            page_summary = f"\n--- è®°å½•æ¥æº: {db_name} | æ ‡é¢˜: {title} ---\n"
            if db_name == "AIå€™é€‰äººåˆ†æä¸­å¿ƒ":
                reason_prop = properties.get("è¯„åˆ†ç†ç”±", {}).get("rich_text", [])
                if reason_prop: page_summary += f"æ ¸å¿ƒè¯„ä»·: {reason_prop[0].get('plain_text', '')}\n"
            else: 
                try:
                    blocks_response = notion.blocks.children.list(block_id=page["id"], page_size=10)
                    for block in blocks_response.get("results", []):
                        if "paragraph" in block and "rich_text" in block["paragraph"]:
                            for text_part in block["paragraph"]["rich_text"]: page_summary += text_part.get("plain_text", "") + "\n"
                except APIResponseError: page_summary += "[æ— æ³•è·å–é¡µé¢æ­£æ–‡]\n"
            content_list.append(page_summary)
        
        # æ‰“å°æœ€ç»ˆæ‹‰å–åˆ°çš„æ€»æ•°
        print(f"  - æˆåŠŸä» [{db_name}] æ‹‰å– {len(all_pages)} æ¡è®°å½•ã€‚")
        return "\n".join(content_list)
        
    except APIResponseError as e:
        print(f"âŒ æŸ¥è¯¢ [{db_name}] æ—¶å‡ºé”™: {e}"); return ""


# --- AI Promptç”Ÿæˆå™¨ (v5.0åŸæ ·ä¿ç•™) ---
def get_prompt_for_report(report_type, data_text, start_date_str, end_date_str, historical_insights=""):
    report_titles = {'daily': 'æ¯æ—¥æˆ˜ç•¥å¤ç›˜ä¸æœªæ¥è§„åˆ’', 'weekly': 'æ¯å‘¨æˆ˜ç•¥å›é¡¾ä¸å±•æœ›', 'monthly': 'æœˆåº¦æˆ˜ç•¥å¤ç›˜ä¸ç›®æ ‡æ ¡å‡†', 'quarterly': 'å­£åº¦æ·±åº¦å¤ç›˜ä¸æˆ˜ç•¥è°ƒæ•´', 'yearly': 'å¹´åº¦ç»¼åˆå¤ç›˜ä¸æœªæ¥æˆ˜ç•¥è§„åˆ’'}
    report_title = report_titles.get(report_type, 'å‘¨æœŸæ€§å¤ç›˜æŠ¥å‘Š')
    period_scopes = {'daily': ('ä»Šæ—¥', 'æ˜æ—¥'), 'weekly': ('æœ¬å‘¨', 'ä¸‹å‘¨'), 'monthly': 'æœ¬æœˆ', 'quarterly': 'æœ¬å­£åº¦', 'yearly': 'æœ¬å¹´åº¦'}
    scope_texts = {'daily': ("ä»Šæ—¥æ ¸å¿ƒæˆæœæ¦‚è§ˆ", "æ˜æ—¥æ ¸å¿ƒä¼˜å…ˆäº‹é¡¹"), 'weekly': ("æœ¬å‘¨æ ¸å¿ƒæˆæœä¸è¶‹åŠ¿", "ä¸‹å‘¨æ ¸å¿ƒç›®æ ‡ä¸ç­–ç•¥"), 'monthly': ("æœ¬æœˆå…³é”®è¿›å±•ä¸æŒ‘æˆ˜", "ä¸‹æœˆæˆ˜ç•¥é‡ç‚¹"), 'quarterly': ("æœ¬å­£åº¦é‡å¤§æˆå°±ä¸ç“¶é¢ˆ", "ä¸‹å­£åº¦æ ¸å¿ƒæˆ˜ç•¥æ–¹å‘"), 'yearly': ("æœ¬å¹´åº¦æ ¸å¿ƒé‡Œç¨‹ç¢‘ä¸æ•™è®­", "ä¸‹ä¸€å¹´åº¦æˆ˜ç•¥è“å›¾")}
    current_scope, next_scope = scope_texts[report_type]
    historical_context_section = ""
    if historical_insights:
        historical_context_section = f"""
# ç›¸å…³å†å²æ´å¯Ÿ (ä»æˆ‘çš„è®°å¿†åº“ä¸­æ£€ç´¢)
è¿™äº›æ˜¯è¿‡å»æœ€ç›¸å…³çš„æŠ¥å‘Šæ‘˜è¦ï¼Œè¯·åœ¨åˆ†ææ—¶å‚è€ƒï¼Œä»¥å‘ç°è¶‹åŠ¿å’Œè”ç³»ï¼š
{historical_insights}
---
"""
    prompt = f"""
# è§’è‰²ä¸ä»»åŠ¡
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ã€å…·æœ‰è¿ç»­è®°å¿†çš„æˆ˜ç•¥é¡¾é—®ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç»“åˆã€ç›¸å…³å†å²æ´å¯Ÿã€‘å’Œæˆ‘æä¾›çš„ã€å½“å‰å‘¨æœŸæ•°æ®ã€‘ï¼Œç”Ÿæˆä¸€ä»½æ·±åˆ»çš„ã€å…·æœ‰å‰åå…³è”æ€§çš„å¤ç›˜æŠ¥å‘Šã€‚

{historical_context_section}

# å½“å‰å¾…åˆ†ææ•°æ®
---
ã€{period_scopes[report_type][0] if isinstance(period_scopes[report_type], tuple) else period_scopes[report_type]}å·¥ä½œæ•°æ®æ±‡æ€» (ä» {start_date_str} åˆ° {end_date_str})ã€‘
{data_text}
---

# è¾“å‡ºæŒ‡ä»¤ä¸æ ¼å¼
ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºï¼Œåˆ†ä¸ºã€æŠ¥å‘Šä¸»ä½“ã€‘å’Œã€è¡ŒåŠ¨æŒ‡é’ˆã€‘ä¸¤éƒ¨åˆ†ã€‚åœ¨åˆ†ææ—¶ï¼Œè¯·ç‰¹åˆ«æ³¨æ„ã€å½“å‰å‘¨æœŸæ•°æ®ã€‘ä¸ã€ç›¸å…³å†å²æ´å¯Ÿã€‘ä¹‹é—´çš„è”ç³»ã€æ¼”å˜æˆ–çŸ›ç›¾ã€‚

### ç¬¬1éƒ¨åˆ†ï¼šæŠ¥å‘Šä¸»ä½“
---
## {report_title} - {end_date_str}
### 1. {current_scope} (Executive Summary)
*   **[ç”¨2-4ä¸ªè¦ç‚¹ï¼Œé«˜åº¦æ¦‚æ‹¬æœ¬å‘¨æœŸå†…çš„æ ¸å¿ƒäº§å‡ºã€å…³é”®è¶‹åŠ¿å’Œæ´»åŠ¨]**
### 2. äº®ç‚¹ä¸é«˜å…‰æ—¶åˆ» (Key Achievements & Highlights)
*   **[è¯†åˆ«å¹¶é˜è¿°1-3ä»¶æœ€æœ‰ä»·å€¼çš„äº‹ï¼Œå¹¶åˆ†æå…¶å¯¹äº {period_scopes[report_type][0] if isinstance(period_scopes[report_type], tuple) else period_scopes[report_type]} ç›®æ ‡çš„æˆ˜ç•¥æ„ä¹‰]**
### 3. ç“¶é¢ˆä¸æˆ˜ç•¥åæ€ (Bottlenecks & Strategic Reflections)
*   **[åˆ†ææœ¬å‘¨æœŸå†…é‡åˆ°çš„ä¸»è¦éšœç¢ã€æ•ˆç‡ç“¶é¢ˆï¼Œå¹¶è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æˆ˜ç•¥åŸå› åæ€ã€‚æ˜¯èµ„æºé—®é¢˜ï¼Ÿæ–¹å‘é—®é¢˜ï¼Ÿè¿˜æ˜¯æ‰§è¡Œé—®é¢˜ï¼Ÿ]**
### 4. è·¨å‘¨æœŸæ´å¯Ÿä¸æ¨¡å¼è¯†åˆ« (Cross-Period Insights & Pattern Recognition)
*   **æµ®ç°çš„ä¸»é¢˜:** [æœ¬å‘¨æœŸå†…åå¤å‡ºç°çš„æ–°ä¸»é¢˜æˆ–æ–°æœºä¼šæ˜¯ä»€ä¹ˆï¼Ÿ]
*   **æ¨¡å¼ä¸å…³è”:** [ä¸åŒå·¥ä½œä»»åŠ¡ä¹‹é—´æ˜¯å¦å­˜åœ¨å¯ä»¥åˆ©ç”¨çš„æ½œåœ¨å…³è”æˆ–æ¨¡å¼ï¼Ÿ]
*   **æˆ˜ç•¥å‡è®¾éªŒè¯:** [æœ¬å‘¨æœŸçš„å®è·µï¼Œæ˜¯éªŒè¯äº†è¿˜æ˜¯æŒ‘æˆ˜äº†æˆ‘ä»¬ä¹‹å‰çš„æˆ˜ç•¥å‡è®¾ï¼Ÿ]
### 5. {next_scope} ({'Priorities for Next Period' if report_type != 'daily' else 'Top Priorities for Tomorrow'})
- [ ] **[åŸºäºä»¥ä¸Šæ‰€æœ‰åˆ†æï¼Œç”Ÿæˆ2-4ä¸ªæœ€é‡è¦ã€æœ€å…·æˆ˜ç•¥ä»·å€¼çš„å¾…åŠäº‹é¡¹æˆ–ç›®æ ‡ï¼Œç”¨äºæŒ‡å¯¼ä¸‹ä¸€å‘¨æœŸ]**
- [ ] **[ç¬¬äºŒä¸ªå¾…åŠäº‹é¡¹]**
---

### ç¬¬2éƒ¨åˆ†ï¼šè¡ŒåŠ¨æŒ‡é’ˆ
åœ¨æŠ¥å‘Šä¸»ä½“ä¹‹åï¼Œä½ å¿…é¡»å¦èµ·ä¸€è¡Œï¼Œæä¾›ä¸€ä¸ªè¢« `<SUMMARY>` å’Œ `</SUMMARY>` æ ‡ç­¾åŒ…è£¹çš„ã€ä¸è¶…è¿‡ä¸¤å¥è¯çš„è¡ŒåŠ¨æŒ‡é’ˆã€‚è¿™ä¸ªæŒ‡é’ˆè¦é«˜åº¦æµ“ç¼©æŠ¥å‘Šä¸­æœ€æ ¸å¿ƒçš„ã€æœ€éœ€è¦æˆ‘å…³æ³¨çš„è¡ŒåŠ¨å»ºè®®ï¼Œä½œä¸ºä¸‹ä¸€å‘¨æœŸçš„æœ€é«˜æŒ‡å¯¼åŸåˆ™ã€‚
ã€ç¤ºä¾‹ã€‘: `<SUMMARY>ä¸‹å‘¨åº”é›†ä¸­ç²¾åŠ›å°†AIå€™é€‰äººåˆ†ææµç¨‹æ ‡å‡†åŒ–ï¼Œå¹¶å¯åŠ¨å¯¹'é¡¹ç›®X'çš„åˆæ­¥æŠ€æœ¯é¢„ç ”ï¼Œä»¥éªŒè¯å…¶é•¿æœŸä»·å€¼ã€‚</SUMMARY>`
"""
    return prompt

# --- è°ƒç”¨AIè¿›è¡Œåˆ†æ (v5.0åŸæ ·ä¿ç•™) ---
def analyze_and_generate_report(gemini_model, prompt):
    print("ğŸ§  æ­£åœ¨è°ƒç”¨AIè¿›è¡Œæ·±åº¦æˆ˜ç•¥åˆ†æ...")
    print("   (æ•°æ®å·²å‘é€ç»™Googleï¼Œè®¾å®š3åˆ†é’Ÿè¶…æ—¶é™åˆ¶ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
    try:
        response = gemini_model.generate_content(prompt, request_options={"timeout": 180})
        report_text = response.text
        print("âœ… AIæˆ˜ç•¥åˆ†æå®Œæˆï¼Œé«˜çº§æŠ¥å‘Šå·²ç”Ÿæˆï¼")
        return report_text
    except Exception as e:
        print(f"âŒ AIåˆ†ææ—¶å‡ºé”™ï¼ç¨‹åºä¸­æ–­ã€‚ é”™è¯¯è¯¦æƒ…: {e}"); return None

# --- å°†æŠ¥å‘Šä¿å­˜å›Notion (v5.0åŸæ ·ä¿ç•™) ---
def save_report_to_notion(notion, memory, config, report_type, report_text, original_data, start_date, end_date):
    review_db_id = config["REVIEW_DB_ID"]
    report_type_map = {'daily': 'AIå¤ç›˜æŠ¥å‘Š', 'weekly': 'AIå‘¨æŠ¥', 'monthly': 'AIæœˆæŠ¥', 'quarterly': 'AIå­£æŠ¥', 'yearly': 'AIå¹´æŠ¥'}
    report_notion_type = report_type_map.get(report_type, 'AIå¤ç›˜æŠ¥å‘Š')
    print(f"âœï¸ æ­£åœ¨å°†ã€{report_notion_type}ã€‘ä¿å­˜åˆ°'æ¯æ—¥å·¥ä½œæ—¥å¿—'æ•°æ®åº“...")
    summary, main_report = "", report_text
    start_tag, end_tag = "<SUMMARY>", "</SUMMARY>"
    start_index, end_index = report_text.find(start_tag), report_text.find(end_tag, report_text.find(start_tag))
    if start_index != -1 and end_index != -1:
        summary = report_text[start_index + len(start_tag):end_index].strip()
        main_report = report_text[:start_index].strip()
        print(f"  - å·²æˆåŠŸæå–è¡ŒåŠ¨æŒ‡é’ˆ: {summary}")
    else:
        print("  - æœªåœ¨AIå“åº”ä¸­æ‰¾åˆ°è¡ŒåŠ¨æŒ‡é’ˆæ ‡ç­¾ï¼Œè¯¥åˆ—å°†ä¸ºç©ºã€‚")
        main_report = report_text
    try:
        end_date_beijing = end_date.astimezone(timezone(timedelta(hours=8)))
        report_title = f"{end_date_beijing.strftime('%Y-%m-%d')} {report_notion_type}"
        properties_data = {
            "æ—¥å¿—æ ‡é¢˜ åç§°": {"title": [{"text": {"content": report_title}}]},
            "æ—¥æœŸ": {"date": {"start": start_date.astimezone(timezone(timedelta(hours=8))).isoformat(), "end": end_date.astimezone(timezone(timedelta(hours=8))).isoformat()}},
            "æ¡ç›®ç±»å‹": {"select": {"name": report_notion_type}},
            "æœ¬æ—¥çŠ¶æ€å·¥ä½œçŠ¶æ€": {"select": {"name": "å·²å®Œæˆ"}},
        }
        if summary: properties_data["è¡ŒåŠ¨æŒ‡é’ˆ"] = {"rich_text": [{"text": {"content": summary}}]}
        children_blocks = [{"object": "block", "type": "callout", "callout": {"rich_text": [{"type": "text", "text": {"content": chunk}}], "icon": {"emoji": "ğŸ¯" if i == 0 else "ğŸ“„"}, "color": "default"}} for i, chunk in enumerate([main_report[i:i + 1990] for i in range(0, len(main_report), 1990)])]
        new_page = notion.pages.create(parent={"database_id": review_db_id}, properties=properties_data, children=children_blocks)
        print(f"ğŸ‰ {report_notion_type}å·²æˆåŠŸä¿å­˜åˆ°Notionï¼")
        new_page_id = new_page.get('id')
        if new_page_id:
            if memory.client:
                memory_metadata = {'type': report_type, 'date': end_date_beijing.strftime('%Y-%m-%d')}
                threading.Thread(target=memory.add_memory, args=(main_report, memory_metadata), daemon=True).start()
            print(">> æ­£åœ¨å¯åŠ¨åå°ä»»åŠ¡ï¼Œå°†æœ¬æ¬¡å¤ç›˜å­˜å…¥ [AIè®­ç»ƒä¸­å¿ƒ] ...")
            threading.Thread(target=write_to_training_hub, args=(notion, "æ‘˜è¦ç”Ÿæˆ", original_data, main_report, 'DailyReview', new_page_id, config), daemon=True).start()
    except APIResponseError as e:
        print(f"âŒ ä¿å­˜åˆ°Notionå¤±è´¥: {e.code} - {e.body}")
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°Notionæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# --- ä¸»ç¨‹åºå…¥å£ (v5.0åŸæ ·ä¿ç•™) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å‘¨æœŸæ€§å¤ç›˜AI v5.1")
    parser.add_argument('--type', type=str, choices=['daily', 'weekly', 'monthly', 'quarterly', 'yearly'], default='daily', help="æŒ‡å®šè¦ç”Ÿæˆçš„æŠ¥å‘Šç±»å‹ (é»˜è®¤: daily)")
    parser.add_argument('--date', type=str, default=None, help="è¡¥å†™æŒ‡å®šæ—¥æœŸçš„æ—¥æŠ¥ï¼Œæ ¼å¼ä¸º YYYY-MM-DDã€‚ä½¿ç”¨æ­¤å‚æ•°æ—¶å°†å¿½ç•¥ --typeã€‚")
    args = parser.parse_args()
    
    notion, gemini_model, memory, config = initialize()
    
    if args.date:
        report_type = 'daily'; start_date, end_date = get_date_range(report_type, specific_date_str=args.date); print(f"æ¨¡å¼åˆ‡æ¢ï¼šè¡¥å†™æ—¥æŠ¥æ¨¡å¼")
    else:
        report_type = args.type; start_date, end_date = get_date_range(report_type)

    beijing_tz = timezone(timedelta(hours=8))
    start_date_str, end_date_str = start_date.astimezone(beijing_tz).strftime('%Y-%m-%d'), end_date.astimezone(beijing_tz).strftime('%Y-%m-%d')
    print_separator()
    print(f"ğŸ“Š æŠ¥å‘Šç±»å‹: {report_type.upper()} | æ•°æ®å‘¨æœŸ: {start_date_str} to {end_date_str}")
    
    log_data = fetch_data_for_period(notion, config["LOG_DB_ID"], "AIäº’åŠ¨æ—¥å¿—", start_date, end_date)
    brain_data = fetch_data_for_period(notion, config["BRAIN_DB_ID"], "AIä½œæˆ˜æŒ‡æŒ¥å®¤", start_date, end_date)
    candidate_data = fetch_data_for_period(notion, config["CANDIDATE_DB_ID"], "AIå€™é€‰äººåˆ†æä¸­å¿ƒ", start_date, end_date)
    
    full_period_data = (f"--- æ•°æ®æ¥æº: AIäº’åŠ¨æ—¥å¿— ---\n{log_data}\n\n"
                        f"--- æ•°æ®æ¥æº: AIä½œæˆ˜æŒ‡æŒ¥å®¤ ---\n{brain_data}\n\n"
                        f"--- æ•°æ®æ¥æº: AIå€™é€‰äººåˆ†æä¸­å¿ƒ ---\n{candidate_data}")
                       
    if len(full_period_data.strip()) < 150: 
        print(f"\nğŸŸ¡ è¯¥å‘¨æœŸå†…æ ¸å¿ƒæ•°æ®åº“æ— è¶³å¤Ÿæ•°æ®å¯ä¾›åˆ†æ ({len(full_period_data.strip())} å­—ç¬¦)ï¼Œç¨‹åºé€€å‡ºã€‚")
    else:
        query_for_memory = f"ä¸ºæˆ‘çš„{report_type}æŠ¥å‘Šï¼Œæ€»ç»“è¿‡å»çš„æ ¸å¿ƒæˆå°±ã€æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚"
        historical_insights = memory.retrieve_memory(query_for_memory)
        prompt = get_prompt_for_report(report_type, full_period_data, start_date_str, end_date_str, historical_insights)
        final_report_with_summary = analyze_and_generate_report(gemini_model, prompt)
        if final_report_with_summary:
            save_report_to_notion(notion, memory, config, report_type, final_report_with_summary, full_period_data, start_date, end_date)
            
    print_separator()
    input("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œè¯·æŒ‰å›è½¦é”®é€€å‡ºã€‚")