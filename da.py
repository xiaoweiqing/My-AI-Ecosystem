# ==============================================================================
#                      å‘¨æœŸæ€§å¤ç›˜AI (Periodic Review AI) v5.5
#                      (Google Search & Vector Memory - å®Œæ•´å…¼å®¹ç‰ˆ)
# ==============================================================================
# ç‰ˆæœ¬è¯´æ˜ v5.5:
# - ã€æ–°å¢-å¤–éƒ¨æ„ŸçŸ¥ã€‘ä»¥éä¾µå…¥å¼æ–¹å¼é›†æˆGoogleæœç´¢ï¼Œå®ç°æƒ…å¢ƒæ„ŸçŸ¥æˆ˜ç•¥å¤ç›˜ã€‚
#   - ã€æ™ºèƒ½æå–ã€‘åœ¨åˆ†æå‰ï¼Œå…ˆç”¨AIä»å‘¨æœŸæ•°æ®ä¸­æå–æœ€æœ‰ä»·å€¼çš„æœç´¢å…³é”®è¯ã€‚
#   - ã€è‡ªåŠ¨æœç´¢ã€‘è‡ªåŠ¨æ‰§è¡ŒGoogleæœç´¢ï¼Œè·å–ä¸æ‚¨å·¥ä½œç›¸å…³çš„æœ€æ–°èµ„è®¯ã€æŠ€æœ¯æ–‡ç« æˆ–ç«å“åŠ¨æ€ã€‚
#   - ã€æ·±åº¦èåˆã€‘å°†æœç´¢åˆ°çš„å¤–éƒ¨æƒ…æŠ¥æ³¨å…¥æœ€ç»ˆçš„åˆ†æPromptï¼Œè®©æŠ¥å‘Šæ›´å…·æˆ˜ç•¥æ·±åº¦ã€‚
# - ã€åŸåˆ™ã€‘ä¸¥æ ¼éµå®ˆä¸åˆ é™¤ä»»ä½•åŸv5.0ä»£ç åŠŸèƒ½ï¼Œæ‰€æœ‰æ–°åŠŸèƒ½å‡ä¸ºæ–°å¢æˆ–åœ¨åŸæµç¨‹ä¸­æ’å…¥ã€‚
# - ã€ä¿ç•™ã€‘Notionå†™å…¥åˆ†å—æœºåˆ¶ã€å‘é‡æ•°æ®åº“é€»è¾‘ã€è¡¥å†™æ—¥æŠ¥åŠŸèƒ½ç­‰å‡ä¿æŒåŸæ ·ã€‚
# - ã€ä¾èµ–æ›´æ–°ã€‘éœ€è¦å®‰è£… `google-api-python-client` (`pip install google-api-python-client`)ã€‚
# - ã€é…ç½®æ›´æ–°ã€‘éœ€è¦åœ¨ .env æ–‡ä»¶ä¸­é…ç½® `GOOGLE_API_KEY` å’Œ `GOOGLE_CSE_ID` (å¯é€‰)ã€‚
# ==============================================================================

import os
import google.generativeai as genai
from notion_client import Client, APIResponseError
from dotenv import load_dotenv
from datetime import datetime, timezone, timedelta
import sys
import re
import threading
import argparse
import chromadb
# ã€ã€ã€ æ–°å¢ä¾èµ– ã€‘ã€‘ã€‘
import json
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# --- åŸæœ‰å‡½æ•°ï¼Œä¿æŒä¸å˜ ---
def print_separator():
    print("\n" + "="*70 + "\n")

# --- åŸæœ‰å‡½æ•°ï¼Œä¿æŒä¸å˜ ---
def clean_text(text):
    """æ–‡æœ¬å‡€åŒ–å™¨ï¼Œç§»é™¤æ— æ•ˆå­—ç¬¦ï¼Œç¡®ä¿APIè°ƒç”¨æˆåŠŸã€‚"""
    if not isinstance(text, str): return ""
    return re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)

# --- åŸæœ‰å‡½æ•°ï¼Œä¿æŒä¸å˜ ---
def write_to_training_hub(notion, task_type, input_text, output_text, source_db_name, source_page_id, config):
    """å°†å¤ç›˜ä»»åŠ¡ä½œä¸ºè®­ç»ƒæ•°æ®å†™å…¥â€œAIè®­ç»ƒä¸­å¿ƒâ€æ•°æ®åº“ã€‚"""
    training_hub_db_id = config.get("TRAINING_HUB_DB_ID")
    if not training_hub_db_id:
        print(">> [è®­ç»ƒä¸­å¿ƒ] æœªé…ç½®æ•°æ®åº“ID (TRAINING_HUB_DATABASE_ID)ï¼Œè·³è¿‡å†™å…¥ã€‚")
        return

    try:
        safe_input_text = clean_text(str(input_text))[:1990]
        safe_output_text = clean_text(str(output_text))[:1990]
        training_title = f"ã€{task_type}ã€‘{datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d')} å‘¨æœŸæ€§æŠ¥å‘Šæ‘˜è¦"
        
        properties_data = {
            "è®­ç»ƒä»»åŠ¡": {"title": [{"text": {"content": training_title}}]},
            "ä»»åŠ¡ç±»å‹": {"select": {"name": task_type}},
            "æºæ•°æ® (Input)": {"rich_text": [{"text": {"content": safe_input_text}}]},
            "ç†æƒ³è¾“å‡º (Output)": {"rich_text": [{"text": {"content": safe_output_text}}]},
            "æ ‡æ³¨çŠ¶æ€": {"select": {"name": "å¾…å®¡æ ¸"}}
        }
        relation_column_map = {'DailyReview': config.get("RELATION_LINK_REVIEW_NAME", "æºé“¾æ¥-æ¯æ—¥å¤ç›˜")}
        if source_db_name in relation_column_map and source_page_id:
            column_to_update = relation_column_map[source_db_name]
            properties_data[column_to_update] = {"relation": [{"id": source_page_id}]}
        
        notion.pages.create(parent={"database_id": training_hub_db_id}, properties=properties_data)
        print(f">> [è®­ç»ƒä¸­å¿ƒ] å·²æˆåŠŸè®°å½•ä¸€æ¡ '{task_type}' è®­ç»ƒæ•°æ®ã€‚")

    except Exception as e:
        print(f"!! [è®­ç»ƒä¸­å¿ƒ] å†™å…¥æ—¶å‡ºé”™: {e}")

# --- åŸæœ‰ç±»ï¼Œä¿æŒä¸å˜ ---
class VectorMemory:
    def __init__(self, path, collection_name):
        self.client = None
        self.collection = None
        if not path or not collection_name:
            print("ğŸŸ¡ [è®°å¿†æ¨¡å—] æœªé…ç½®CHROMA_DB_PATHæˆ–CHROMA_COLLECTION_NAMEï¼Œå°†è·³è¿‡æ‰€æœ‰å‘é‡æ“ä½œã€‚")
            return
        try:
            print(f"ğŸ§  [è®°å¿†æ¨¡å—] æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
            self.client = chromadb.PersistentClient(path=path)
            self.collection = self.client.get_or_create_collection(name=collection_name, metadata={"hnsw:space": "cosine"})
            print(f"  - æˆåŠŸè¿æ¥åˆ°é›†åˆ '{collection_name}'ã€‚")
        except Exception as e:
            print(f"âŒ [è®°å¿†æ¨¡å—] åˆå§‹åŒ–ChromaDBå¤±è´¥: {e}")
            self.client = None

    def add_memory(self, report_text, metadata):
        if not self.collection: return
        try:
            doc_id = f"{metadata['type']}-{metadata['date']}"
            print(f"  - æ­£åœ¨å°†æŠ¥å‘Š '{doc_id}' å‘é‡åŒ–å¹¶å­˜å…¥é•¿æœŸè®°å¿†...")
            embedding_model = 'models/embedding-001'
            embedding = genai.embed_content(model=embedding_model, content=report_text, task_type="RETRIEVAL_DOCUMENT")["embedding"]
            
            self.collection.upsert(
                ids=[doc_id],
                embeddings=[embedding],
                documents=[report_text],
                metadatas=[metadata]
            )
            print(f"  - âœ… è®°å¿†èƒ¶å›Š '{doc_id}' å·²æˆåŠŸå­˜å…¥å‘é‡æ•°æ®åº“ï¼")
        except Exception as e:
            print(f"  - âŒ [è®°å¿†æ¨¡å—] å­˜å…¥è®°å¿†æ—¶å‡ºé”™: {e}")

    def retrieve_memory(self, query_text, n_results=3):
        if not self.collection: return ""
        try:
            print(f"  - ğŸ§  æ­£åœ¨åŸºäºé—®é¢˜ '{query_text[:30]}...' æ£€ç´¢ç›¸å…³å†å²è®°å¿†...")
            embedding_model = 'models/embedding-001'
            query_embedding = genai.embed_content(model=embedding_model, content=query_text, task_type="RETRIEVAL_QUERY")["embedding"]

            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            if not results or not results['documents'] or not results['documents'][0]:
                print("  - æœªæ‰¾åˆ°ç›¸å…³å†å²è®°å¿†ã€‚")
                return ""

            retrieved_docs = []
            for i, doc in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i]
                entry = f"--- å†å²æ´å¯Ÿ ({meta.get('date', 'æœªçŸ¥æ—¥æœŸ')} {meta.get('type', 'æŠ¥å‘Š')}):\n{doc}\n---"
                retrieved_docs.append(entry)
            
            print(f"  - âœ… æˆåŠŸæ£€ç´¢åˆ° {len(retrieved_docs)} æ¡ç›¸å…³å†å²è®°å¿†ã€‚")
            return "\n".join(retrieved_docs)
        except Exception as e:
            print(f"  - âŒ [è®°å¿†æ¨¡å—] æ£€ç´¢è®°å¿†æ—¶å‡ºé”™: {e}")
            return ""

# --- ã€ã€ã€ æ–°å¢ï¼šGoogleæœç´¢æ¨¡å— ã€‘ã€‘ã€‘ ---
def perform_google_search(query: str, api_key: str, cse_id: str) -> str:
    """æ‰§è¡ŒGoogleæœç´¢å¹¶è¿”å›æ ¼å¼åŒ–çš„ç»“æœæ‘˜è¦ã€‚"""
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        res = service.cse().list(q=query, cx=cse_id, num=3).execute() # æœç´¢å‰3ä¸ªç»“æœ
        if 'items' not in res:
            return f"å¯¹äºæŸ¥è¯¢ '{query}'ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
        snippets = [f"- {item['title']}\n  {item.get('snippet', 'æ— æ‘˜è¦')}" for item in res['items']]
        return f"--- å…³äº '{query}' çš„æœç´¢ç»“æœ ---\n" + "\n".join(snippets)
    except HttpError as e:
        return f"Googleæœç´¢APIé”™è¯¯: {e.reason}"
    except Exception as e:
        return f"æ‰§è¡ŒGoogleæœç´¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"

def extract_search_queries_from_data(gemini_model, data_text):
    """ä½¿ç”¨AIä»åŸå§‹æ•°æ®ä¸­æå–å‡ºæœ€æœ‰ä»·å€¼çš„Googleæœç´¢å…³é”®è¯ã€‚"""
    print("  - ğŸ¤– æ­£åœ¨åˆ†æå‘¨æœŸæ•°æ®ï¼Œæå–æœ€æœ‰ä»·å€¼çš„æœç´¢å…³é”®è¯...")
    try:
        prompt = f"""
        # ä»»åŠ¡
        åˆ†æä»¥ä¸‹å·¥ä½œæ—¥å¿—ï¼Œè¯†åˆ«å‡º2-3ä¸ªæœ€å…·æœ‰ç ”ç©¶ä»·å€¼çš„æ ¸å¿ƒä¸»é¢˜ã€æŠ€æœ¯ã€å…¬å¸åæˆ–é‡åˆ°çš„é—®é¢˜ã€‚
        è¿™äº›å…³é”®è¯åº”è¯¥èƒ½é€šè¿‡Googleæœç´¢ï¼Œä¸ºå³å°†ç”Ÿæˆçš„å¤ç›˜æŠ¥å‘Šå¸¦æ¥æœ€å¤§çš„å¤–éƒ¨è§†è§’å’Œä»·å€¼ã€‚

        # è§„åˆ™
        - åªå…³æ³¨é‚£äº›é€šè¿‡å¤–éƒ¨ä¿¡æ¯èƒ½å¾—åˆ°å¢å¼ºçš„ä¸»é¢˜ã€‚
        - å¿½ç•¥æ—¥å¸¸çäº‹ã€‚
        - ä»¥JSONåˆ—è¡¨çš„å½¢å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š["AI Agentæœ€æ–°è¿›å±•", "ç«å“å…¬å¸XåŠ¨æ€", "Python uvloopæ€§èƒ½ä¼˜åŒ–"]

        # å·¥ä½œæ—¥å¿—
        {data_text[:4000]}
        """
        response = gemini_model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
        queries = json.loads(response.text)
        if isinstance(queries, list) and queries:
            print(f"  - âœ… æˆåŠŸæå–åˆ°å…³é”®è¯: {queries}")
            return queries
        return []
    except Exception as e:
        print(f"  - âŒ æå–æœç´¢å…³é”®è¯å¤±è´¥: {e}")
        return []

# --- 1. åˆå§‹åŒ–ä¸é…ç½®åŠ è½½ (å·²ä¿®æ”¹ï¼Œå¢åŠ Googleæœç´¢ç›¸å…³é€»è¾‘) ---
def initialize():
    """åŠ è½½é…ç½®å¹¶åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
    print_separator()
    print("ğŸš€ å¯åŠ¨å‘¨æœŸæ€§å¤ç›˜AIå¼•æ“ (v5.5 - å®Œæ•´å…¼å®¹ç‰ˆ)...")
    load_dotenv()
    config = {
        "NOTION_TOKEN": os.getenv("NOTION_API_KEY"),
        "API_KEY": os.getenv("GEMINI_API_KEY"),
        "LOG_DB_ID": os.getenv("TOOLBOX_LOG_DATABASE_ID"),
        "BRAIN_DB_ID": os.getenv("CORE_BRAIN_DATABASE_ID"),
        "CANDIDATE_DB_ID": os.getenv("CANDIDATE_DATABASE_ID"),
        "REVIEW_DB_ID": os.getenv("DAILY_REVIEW_DATABASE_ID"),
        "TRAINING_HUB_DB_ID": os.getenv("TRAINING_HUB_DATABASE_ID"),
        "RELATION_LINK_REVIEW_NAME": os.getenv("RELATION_LINK_REVIEW_NAME", "æºé“¾æ¥-æ¯æ—¥å¤ç›˜"),
        "CHROMA_DB_PATH": os.getenv("CHROMA_DB_PATH"),
        "CHROMA_COLLECTION_NAME": os.getenv("CHROMA_COLLECTION_NAME"),
        # ã€ã€ã€ æ–°å¢é…ç½®é¡¹è¯»å– ã€‘ã€‘ã€‘
        "GOOGLE_API_KEY": os.getenv("GOOGLE_API_KEY"),
        "GOOGLE_CSE_ID": os.getenv("GOOGLE_CSE_ID"),
    }
    if not all([config["NOTION_TOKEN"], config["API_KEY"], config["REVIEW_DB_ID"]]):
        print("âŒ é”™è¯¯ï¼šå…³é”®é…ç½®ç¼ºå¤±ï¼(NOTION_API_KEY, GEMINI_API_KEY, DAILY_REVIEW_DATABASE_ID å¿…é¡»åœ¨ .env æ–‡ä»¶ä¸­é…ç½®)")
        sys.exit(1)
    
    # ã€ã€ã€ æ–°å¢é…ç½®æ£€æŸ¥ ã€‘ã€‘ã€‘
    config["ENABLE_GOOGLE_SEARCH"] = bool(config["GOOGLE_API_KEY"] and config["GOOGLE_CSE_ID"])
    if not config["ENABLE_GOOGLE_SEARCH"]:
        print("ğŸŸ¡ [è­¦å‘Š] Googleæœç´¢æœªé…ç½® (GOOGLE_API_KEY, GOOGLE_CSE_ID)ï¼Œå¤–éƒ¨æƒ…æŠ¥æ¨¡å—å°†ç¦ç”¨ã€‚")

    try:
        notion = Client(auth=config["NOTION_TOKEN"])
        genai.configure(api_key=config["API_KEY"])
        # ã€ã€ã€ ä¿®æ”¹ï¼šä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºå¿«é€Ÿä»»åŠ¡ï¼Œä¸€ä¸ªç”¨äºæ·±åº¦åˆ†æ ã€‘ã€‘ã€‘
        gemini_flash_model = genai.GenerativeModel('models/gemini-2.5-flash')
        gemini_pro_model = genai.GenerativeModel('models/gemini-2.5-pro') # ä¿æŒåŸæœ‰çš„Proæ¨¡å‹
        memory = VectorMemory(config["CHROMA_DB_PATH"], config["CHROMA_COLLECTION_NAME"])
        print("âœ… Notion, Gemini (Flash & Pro) å’Œ å‘é‡è®°å¿†æ¨¡å— åˆå§‹åŒ–æˆåŠŸï¼")
        # ã€ã€ã€ ä¿®æ”¹ï¼šè¿”å›æ›´å¤šæ¨¡å‹å’Œå®Œæ•´çš„config ã€‘ã€‘ã€‘
        return notion, gemini_flash_model, gemini_pro_model, memory, config
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"); sys.exit(1)

# --- åŸæœ‰å‡½æ•°ï¼Œä¿æŒä¸å˜ ---
def get_date_range(report_type, specific_date_str=None):
    if specific_date_str:
        try:
            beijing_tz = timezone(timedelta(hours=8))
            target_date = datetime.strptime(specific_date_str, '%Y-%m-%d')
            start_date_local = target_date.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=beijing_tz)
            end_date_local = target_date.replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=beijing_tz)
            return start_date_local.astimezone(timezone.utc), end_date_local.astimezone(timezone.utc)
        except ValueError:
            print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: '{specific_date_str}'ã€‚è¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼ã€‚ç¨‹åºé€€å‡ºã€‚")
            sys.exit(1)
    end_date = datetime.now(timezone.utc)
    today_beijing = datetime.now(timezone(timedelta(hours=8))).date()
    start_of_day_beijing = datetime.combine(today_beijing, datetime.min.time(), tzinfo=timezone(timedelta(hours=8)))
    if report_type == 'daily':
        start_date = start_of_day_beijing.astimezone(timezone.utc)
    elif report_type == 'weekly':
        start_date = end_date - timedelta(days=7)
    elif report_type == 'monthly':
        start_date = end_date - timedelta(days=30)
    elif report_type == 'quarterly':
        start_date = end_date - timedelta(days=90)
    elif report_type == 'yearly':
        start_date = end_date - timedelta(days=365)
    else: # é»˜è®¤ä¸ºæ—¥æŠ¥
        start_date = start_of_day_beijing.astimezone(timezone.utc)
    return start_date, end_date

# --- åŸæœ‰å‡½æ•°ï¼Œä¿æŒä¸å˜ ---
def fetch_data_for_period(notion, db_id, db_name, start_date, end_date):
    if not db_id:
        print(f"ğŸŸ¡ è·³è¿‡ [{db_name}]ï¼šæœªåœ¨.envä¸­é…ç½®å…¶æ•°æ®åº“IDã€‚"); return ""
    print(f"â³ æ­£åœ¨ä» [{db_name}] æ•°æ®åº“æ‹‰å–å‘¨æœŸå†…æ•°æ®...")
    try:
        response = notion.databases.query(
            database_id=db_id, 
            filter={
                "and": [
                    {"timestamp": "last_edited_time", "last_edited_time": {"on_or_after": start_date.isoformat()}},
                    {"timestamp": "last_edited_time", "last_edited_time": {"before": end_date.isoformat()}}
                ]
            }
        )
        pages = response.get("results", [])
        if not pages:
            print(f"  - åœ¨ [{db_name}] ä¸­æœªå‘ç°è¯¥å‘¨æœŸå†…çš„æ›´æ–°ã€‚"); return ""
        content_list = []
        for page in pages:
            title, properties = "[æ— æ ‡é¢˜]", page.get("properties", {})
            title_prop_names = ["ä¸»é¢˜", "æ—¥å¿—æ ‡é¢˜ åç§°", "å€™é€‰äººå§“å"]
            for prop_name in title_prop_names:
                if prop_name in properties and properties[prop_name].get("type") == "title":
                    title_parts = properties[prop_name].get("title", [])
                    if title_parts: title = title_parts[0].get("plain_text", "[ç©ºæ ‡é¢˜]"); break
            page_summary = f"\n--- è®°å½•æ¥æº: {db_name} | æ ‡é¢˜: {title} ---\n"
            if db_name == "AIå€™é€‰äººåˆ†æä¸­å¿ƒ":
                reason_prop = properties.get("è¯„åˆ†ç†ç”±", {}).get("rich_text", [])
                if reason_prop: page_summary += f"æ ¸å¿ƒè¯„ä»·: {reason_prop[0].get('plain_text', '')}\n"
            else: 
                try:
                    blocks_response = notion.blocks.children.list(block_id=page["id"], page_size=10)
                    for block in blocks_response.get("results", []):
                        if "paragraph" in block and "rich_text" in block["paragraph"]:
                            for text_part in block["paragraph"]["rich_text"]: page_summary += text_part.get("plain_text", "") + "\n"
                except APIResponseError: page_summary += "[æ— æ³•è·å–é¡µé¢æ­£æ–‡]\n"
            content_list.append(page_summary)
        print(f"  - æˆåŠŸä» [{db_name}] æ‹‰å– {len(pages)} æ¡è®°å½•ã€‚")
        return "\n".join(content_list)
    except APIResponseError as e:
        print(f"âŒ æŸ¥è¯¢ [{db_name}] æ—¶å‡ºé”™: {e}"); return ""

# --- AI Promptç”Ÿæˆå™¨ (å·²ä¿®æ”¹ï¼Œå¢åŠ Googleæœç´¢ç»“æœæ³¨å…¥) ---
def get_prompt_for_report(report_type, data_text, start_date_str, end_date_str, historical_insights="", google_search_summary=""):
    """æ ¹æ®æŠ¥å‘Šç±»å‹ç”Ÿæˆä¸“å±çš„AI Prompt, å¹¶æ³¨å…¥å†å²æ´å¯Ÿå’ŒGoogleæœç´¢ç»“æœ"""
    # --- åŸæœ‰é€»è¾‘ ---
    report_titles = {'daily': 'æ¯æ—¥æˆ˜ç•¥å¤ç›˜ä¸æœªæ¥è§„åˆ’', 'weekly': 'æ¯å‘¨æˆ˜ç•¥å›é¡¾ä¸å±•æœ›', 'monthly': 'æœˆåº¦æˆ˜ç•¥å¤ç›˜ä¸ç›®æ ‡æ ¡å‡†', 'quarterly': 'å­£åº¦æ·±åº¦å¤ç›˜ä¸æˆ˜ç•¥è°ƒæ•´', 'yearly': 'å¹´åº¦ç»¼åˆå¤ç›˜ä¸æœªæ¥æˆ˜ç•¥è§„åˆ’'}
    report_title = report_titles.get(report_type, 'å‘¨æœŸæ€§å¤ç›˜æŠ¥å‘Š')
    period_scopes = {'daily': ('ä»Šæ—¥', 'æ˜æ—¥'), 'weekly': ('æœ¬å‘¨', 'ä¸‹å‘¨'), 'monthly': ('æœ¬æœˆ', 'ä¸‹æœˆ'), 'quarterly': ('æœ¬å­£åº¦', 'ä¸‹å­£åº¦'), 'yearly': ('æœ¬å¹´åº¦', 'ä¸‹å¹´åº¦')}
    scope_texts = {'daily': ("ä»Šæ—¥æ ¸å¿ƒæˆæœæ¦‚è§ˆ", "æ˜æ—¥æ ¸å¿ƒä¼˜å…ˆäº‹é¡¹"), 'weekly': ("æœ¬å‘¨æ ¸å¿ƒæˆæœä¸è¶‹åŠ¿", "ä¸‹å‘¨æ ¸å¿ƒç›®æ ‡ä¸ç­–ç•¥"), 'monthly': ("æœ¬æœˆå…³é”®è¿›å±•ä¸æŒ‘æˆ˜", "ä¸‹æœˆæˆ˜ç•¥é‡ç‚¹"), 'quarterly': ("æœ¬å­£åº¦é‡å¤§æˆå°±ä¸ç“¶é¢ˆ", "ä¸‹å­£åº¦æ ¸å¿ƒæˆ˜ç•¥æ–¹å‘"), 'yearly': ("æœ¬å¹´åº¦æ ¸å¿ƒé‡Œç¨‹ç¢‘ä¸æ•™è®­", "ä¸‹ä¸€å¹´åº¦æˆ˜ç•¥è“å›¾")}
    current_scope, next_scope = scope_texts[report_type]
    
    # --- ã€ã€ã€ ä¿®æ”¹ï¼šå°†åŸæœ‰çš„å†å²æ´å¯Ÿéƒ¨åˆ†å’Œæ–°å¢çš„å¤–éƒ¨æƒ…æŠ¥éƒ¨åˆ†åˆå¹¶ï¼Œå¢å¼ºç»“æ„ ã€‘ã€‘ã€‘ ---
    external_context_section = f"""# å®æ—¶å¤–éƒ¨æƒ…æŠ¥ (æ¥è‡ªGoogleæœç´¢)
{google_search_summary if google_search_summary else "æœ¬æ¬¡æœªè¿›è¡Œå¤–éƒ¨æƒ…æŠ¥æœç´¢ã€‚"}
---
""" if google_search_summary else ""
    
    historical_context_section = f"""# ç›¸å…³å†å²æ´å¯Ÿ (ä»æˆ‘çš„è®°å¿†åº“ä¸­æ£€ç´¢)
{historical_insights if historical_insights else "æœªæ£€ç´¢åˆ°ç›¸å…³å†å²è®°å¿†ã€‚"}
---
"""
    # --- ã€ã€ã€ ä¿®æ”¹ï¼šæ›´æ–°Promptæ¨¡æ¿ï¼Œä½¿å…¶ç»“æ„æ›´æ¸…æ™°ï¼Œå¹¶åŒ…å«æ–°æ¨¡å— ã€‘ã€‘ã€‘ ---
    prompt = f"""
# è§’è‰²ä¸ä»»åŠ¡
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ã€å…·å¤‡â€œå†…éƒ¨è®°å¿†â€å’Œâ€œå¤–éƒ¨æ„ŸçŸ¥â€èƒ½åŠ›çš„æˆ˜ç•¥é¡¾é—®ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç»“åˆã€å®æ—¶å¤–éƒ¨æƒ…æŠ¥ã€‘(å¦‚æœæä¾›)ã€ã€ç›¸å…³å†å²æ´å¯Ÿã€‘(å¦‚æœæä¾›)å’Œæˆ‘æä¾›çš„ã€å½“å‰å‘¨æœŸæ•°æ®ã€‘ï¼Œç”Ÿæˆä¸€ä»½æå…·æ·±åº¦å’Œå‰ç»æ€§çš„å¤ç›˜æŠ¥å‘Šã€‚

{external_context_section}
{historical_context_section}

# å½“å‰å¾…åˆ†ææ•°æ®
---
ã€{period_scopes[report_type][0] if isinstance(period_scopes[report_type], tuple) else period_scopes[report_type]}å·¥ä½œæ•°æ®æ±‡æ€» (ä» {start_date_str} åˆ° {end_date_str})ã€‘
{data_text}
---

# è¾“å‡ºæŒ‡ä»¤ä¸æ ¼å¼
ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºã€‚åœ¨åˆ†ææ—¶ï¼Œè¯·ç‰¹åˆ«æ³¨æ„å°†ã€å¤–éƒ¨æƒ…æŠ¥ã€‘å’Œã€å†å²æ´å¯Ÿã€‘ä¸ã€å½“å‰æ•°æ®ã€‘è¿›è¡Œäº¤å‰å¼•ç”¨å’Œæ·±åº¦æ€è€ƒã€‚

### ç¬¬1éƒ¨åˆ†ï¼šæŠ¥å‘Šä¸»ä½“
---
## {report_title} - {end_date_str}
### 1. {current_scope} (Executive Summary)
*   **[ç”¨2-4ä¸ªè¦ç‚¹ï¼Œé«˜åº¦æ¦‚æ‹¬æœ¬å‘¨æœŸå†…çš„æ ¸å¿ƒäº§å‡ºã€å…³é”®è¶‹åŠ¿å’Œæ´»åŠ¨]**
### 2. äº®ç‚¹ä¸é«˜å…‰æ—¶åˆ» (Key Achievements & Highlights)
*   **[è¯†åˆ«å¹¶é˜è¿°1-3ä»¶æœ€æœ‰ä»·å€¼çš„äº‹ï¼Œå¹¶ç»“åˆå¤–éƒ¨ä¿¡æ¯åˆ†æå…¶å¯¹äº {period_scopes[report_type][0] if isinstance(period_scopes[report_type], tuple) else period_scopes[report_type]} ç›®æ ‡çš„æˆ˜ç•¥æ„ä¹‰]**
### 3. ç“¶é¢ˆä¸æˆ˜ç•¥åæ€ (Bottlenecks & Strategic Reflections)
*   **[åˆ†ææœ¬å‘¨æœŸå†…é‡åˆ°çš„ä¸»è¦éšœç¢ã€æ•ˆç‡ç“¶é¢ˆï¼Œå¹¶è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æˆ˜ç•¥åŸå› åæ€ã€‚æ˜¯èµ„æºé—®é¢˜ï¼Ÿæ–¹å‘é—®é¢˜ï¼Ÿè¿˜æ˜¯æ‰§è¡Œé—®é¢˜ï¼Ÿ]**
### 4. è·¨å‘¨æœŸæ´å¯Ÿä¸æ¨¡å¼è¯†åˆ« (Cross-Period Insights & Pattern Recognition)
*   **æµ®ç°çš„ä¸»é¢˜:** [æœ¬å‘¨æœŸå†…åå¤å‡ºç°çš„æ–°ä¸»é¢˜æˆ–æ–°æœºä¼šæ˜¯ä»€ä¹ˆï¼Ÿå¤–éƒ¨ä¸–ç•Œæ˜¯å¦ä¹Ÿåœ¨è®¨è®ºè¿™ä¸ªä¸»é¢˜ï¼Ÿ]
*   **æ¨¡å¼ä¸å…³è”:** [ä¸åŒå·¥ä½œä»»åŠ¡ä¹‹é—´æ˜¯å¦å­˜åœ¨å¯ä»¥åˆ©ç”¨çš„æ½œåœ¨å…³è”æˆ–æ¨¡å¼ï¼Ÿå†å²æ´å¯Ÿæ˜¯å¦æ­ç¤ºäº†é‡å¤å‡ºç°çš„æ¨¡å¼ï¼Ÿ]
*   **æˆ˜ç•¥å‡è®¾éªŒè¯:** [æœ¬å‘¨æœŸçš„å®è·µï¼Œæ˜¯éªŒè¯äº†è¿˜æ˜¯æŒ‘æˆ˜äº†æˆ‘ä»¬ä¹‹å‰çš„æˆ˜ç•¥å‡è®¾ï¼Ÿå¤–éƒ¨æƒ…æŠ¥æ˜¯å¦æä¾›äº†æ–°çš„è§†è§’ï¼Ÿ]
### 5. {next_scope} ({'Priorities for Next Period' if report_type != 'daily' else 'Top Priorities for Tomorrow'})
- [ ] **[åŸºäºä»¥ä¸Šæ‰€æœ‰åˆ†æï¼Œç‰¹åˆ«æ˜¯å¤–éƒ¨æƒ…æŠ¥çš„å¯å‘ï¼Œç”Ÿæˆ2-4ä¸ªæœ€å…·æˆ˜ç•¥ä»·å€¼çš„å¾…åŠäº‹é¡¹æˆ–ç›®æ ‡ï¼Œç”¨äºæŒ‡å¯¼ä¸‹ä¸€å‘¨æœŸ]**
- [ ] **[ç¬¬äºŒä¸ªå¾…åŠäº‹é¡¹]**
---

### ç¬¬2éƒ¨åˆ†ï¼šè¡ŒåŠ¨æŒ‡é’ˆ
åœ¨æŠ¥å‘Šä¸»ä½“ä¹‹åï¼Œä½ å¿…é¡»å¦èµ·ä¸€è¡Œï¼Œæä¾›ä¸€ä¸ªè¢« `<SUMMARY>` å’Œ `</SUMMARY>` æ ‡ç­¾åŒ…è£¹çš„ã€ä¸è¶…è¿‡ä¸¤å¥è¯çš„è¡ŒåŠ¨æŒ‡é’ˆã€‚è¿™ä¸ªæŒ‡é’ˆè¦é«˜åº¦æµ“ç¼©æŠ¥å‘Šä¸­æœ€æ ¸å¿ƒçš„ã€æœ€éœ€è¦æˆ‘å…³æ³¨çš„è¡ŒåŠ¨å»ºè®®ï¼Œä½œä¸ºä¸‹ä¸€å‘¨æœŸçš„æœ€é«˜æŒ‡å¯¼åŸåˆ™ã€‚
ã€ç¤ºä¾‹ã€‘: `<SUMMARY>ä¸‹å‘¨åº”é›†ä¸­ç²¾åŠ›å°†AIå€™é€‰äººåˆ†ææµç¨‹æ ‡å‡†åŒ–ï¼Œå¹¶å¯åŠ¨å¯¹'é¡¹ç›®X'çš„åˆæ­¥æŠ€æœ¯é¢„ç ”ï¼Œä»¥éªŒè¯å…¶é•¿æœŸä»·å€¼ã€‚</SUMMARY>`
"""
    return prompt

# --- åŸæœ‰å‡½æ•°ï¼Œä¿æŒä¸å˜ ---
def analyze_and_generate_report(gemini_model, prompt):
    print("ğŸ§  æ­£åœ¨è°ƒç”¨AIè¿›è¡Œæ·±åº¦æˆ˜ç•¥åˆ†æ...")
    print("   (æ•°æ®å·²å‘é€ç»™Googleï¼Œè®¾å®š3åˆ†é’Ÿè¶…æ—¶é™åˆ¶ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
    try:
        response = gemini_model.generate_content(prompt, request_options={"timeout": 180})
        report_text = response.text
        print("âœ… AIæˆ˜ç•¥åˆ†æå®Œæˆï¼Œé«˜çº§æŠ¥å‘Šå·²ç”Ÿæˆï¼")
        return report_text
    except Exception as e:
        print(f"âŒ AIåˆ†ææ—¶å‡ºé”™ï¼ç¨‹åºä¸­æ–­ã€‚ é”™è¯¯è¯¦æƒ…: {e}"); return None

# --- åŸæœ‰å‡½æ•°ï¼Œä¿æŒä¸å˜ï¼Œç‰¹åˆ«æ˜¯children_blocksçš„åˆ†å—é€»è¾‘ ---
def save_report_to_notion(notion, memory, config, report_type, report_text, original_data, start_date, end_date):
    """å°†æŠ¥å‘Šä¿å­˜åˆ°Notionï¼Œå¹¶è§¦å‘å‘é‡åŒ–å’Œè®­ç»ƒä¸­å¿ƒå†™å…¥ã€‚"""
    review_db_id = config["REVIEW_DB_ID"]
    report_type_map = { 'daily': 'AIå¤ç›˜æŠ¥å‘Š', 'weekly': 'AIå‘¨æŠ¥', 'monthly': 'AIæœˆæŠ¥', 'quarterly': 'AIå­£æŠ¥', 'yearly': 'AIå¹´æŠ¥' }
    report_notion_type = report_type_map.get(report_type, 'AIå¤ç›˜æŠ¥å‘Š')
    print(f"âœï¸ æ­£åœ¨å°†ã€{report_notion_type}ã€‘ä¿å­˜åˆ°'æ¯æ—¥å·¥ä½œæ—¥å¿—'æ•°æ®åº“...")
    summary, main_report = "", report_text
    start_tag, end_tag = "<SUMMARY>", "</SUMMARY>"
    start_index, end_index = report_text.find(start_tag), report_text.find(end_tag, report_text.find(start_tag))
    if start_index != -1 and end_index != -1:
        summary = report_text[start_index + len(start_tag):end_index].strip()
        main_report = report_text[:start_index].strip()
        print(f"  - å·²æˆåŠŸæå–è¡ŒåŠ¨æŒ‡é’ˆ: {summary}")
    else:
        print("  - æœªåœ¨AIå“åº”ä¸­æ‰¾åˆ°è¡ŒåŠ¨æŒ‡é’ˆæ ‡ç­¾ï¼Œè¯¥åˆ—å°†ä¸ºç©ºã€‚")
        main_report = report_text
    try:
        end_date_beijing = end_date.astimezone(timezone(timedelta(hours=8)))
        report_title = f"{end_date_beijing.strftime('%Y-%m-%d')} {report_notion_type}"
        properties_data = {
            "æ—¥å¿—æ ‡é¢˜ åç§°": {"title": [{"text": {"content": report_title}}]},
            "æ—¥æœŸ": {"date": {"start": start_date.astimezone(timezone(timedelta(hours=8))).isoformat(), "end": end_date.astimezone(timezone(timedelta(hours=8))).isoformat()}},
            "æ¡ç›®ç±»å‹": {"select": {"name": report_notion_type}},
            "æœ¬æ—¥çŠ¶æ€å·¥ä½œçŠ¶æ€": {"select": {"name": "å·²å®Œæˆ"}},
        }
        if summary:
            properties_data["è¡ŒåŠ¨æŒ‡é’ˆ"] = {"rich_text": [{"text": {"content": summary}}]}
        
        # --- ã€ã€ã€ æ‚¨æˆç†Ÿå¥½ç”¨çš„åˆ†å—é€»è¾‘ï¼ŒåŸå°ä¸åŠ¨ ã€‘ã€‘ã€‘ ---
        children_blocks = [{
            "object": "block", "type": "callout",
            "callout": {
                "rich_text": [{"type": "text", "text": {"content": chunk}}],
                "icon": {"emoji": "ğŸ¯" if i == 0 else "ğŸ“„"},
                "color": "default"
            }
        } for i, chunk in enumerate([main_report[i:i + 1990] for i in range(0, len(main_report), 1990)])]

        new_page = notion.pages.create(parent={"database_id": review_db_id}, properties=properties_data, children=children_blocks)
        print(f"ğŸ‰ {report_notion_type}å·²æˆåŠŸä¿å­˜åˆ°Notionï¼")
        new_page_id = new_page.get('id')
        if new_page_id:
            if memory.client:
                memory_metadata = {'type': report_type, 'date': end_date_beijing.strftime('%Y-%m-%d')}
                threading.Thread(target=memory.add_memory, args=(main_report, memory_metadata), daemon=True).start()
            print(">> æ­£åœ¨å¯åŠ¨åå°ä»»åŠ¡ï¼Œå°†æœ¬æ¬¡å¤ç›˜å­˜å…¥ [AIè®­ç»ƒä¸­å¿ƒ] ...")
            threading.Thread(target=write_to_training_hub,args=(notion, "æ‘˜è¦ç”Ÿæˆ", original_data, main_report, 'DailyReview', new_page_id, config),daemon=True).start()
    except APIResponseError as e:
        print(f"âŒ ä¿å­˜åˆ°Notionå¤±è´¥: {e.code} - {e.body}")
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°Notionæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# --- ä¸»ç¨‹åºå…¥å£ (å·²ä¿®æ”¹ï¼Œæ’å…¥äº†Googleæœç´¢æµç¨‹) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å‘¨æœŸæ€§å¤ç›˜AI v5.5 - å®Œæ•´å…¼å®¹ç‰ˆ")
    parser.add_argument('--type', type=str, choices=['daily', 'weekly', 'monthly', 'quarterly', 'yearly'], default='daily', help="æŒ‡å®šè¦ç”Ÿæˆçš„æŠ¥å‘Šç±»å‹ (é»˜è®¤: daily)")
    parser.add_argument('--date', type=str, default=None, help="è¡¥å†™æŒ‡å®šæ—¥æœŸçš„æ—¥æŠ¥ï¼Œæ ¼å¼ä¸º YYYY-MM-DDã€‚")
    args = parser.parse_args()
    
    # ã€ä¿®æ”¹ã€‘åˆå§‹åŒ–è°ƒç”¨ä¼šè¿”å›æ›´å¤šæ¨¡å‹
    notion, gemini_flash_model, gemini_pro_model, memory, config = initialize()
    
    # 1. ç¡®å®šæŠ¥å‘Šç±»å‹å’Œæ—¶é—´èŒƒå›´ (åŸæœ‰é€»è¾‘)
    if args.date:
        report_type = 'daily'
        start_date, end_date = get_date_range(report_type, specific_date_str=args.date)
        print(f"æ¨¡å¼åˆ‡æ¢ï¼šè¡¥å†™æ—¥æŠ¥æ¨¡å¼")
    else:
        report_type = args.type
        start_date, end_date = get_date_range(report_type)
    beijing_tz = timezone(timedelta(hours=8))
    start_date_str = start_date.astimezone(beijing_tz).strftime('%Y-%m-%d')
    end_date_str = end_date.astimezone(beijing_tz).strftime('%Y-%m-%d')
    print_separator()
    print(f"ğŸ“Š æŠ¥å‘Šç±»å‹: {report_type.upper()} | æ•°æ®å‘¨æœŸ: {start_date_str} to {end_date_str}")
    
    # 2. æ‹‰å–å‘¨æœŸå†…æ•°æ® (åŸæœ‰é€»è¾‘)
    log_data = fetch_data_for_period(notion, config["LOG_DB_ID"], "AIäº’åŠ¨æ—¥å¿—", start_date, end_date)
    brain_data = fetch_data_for_period(notion, config["BRAIN_DB_ID"], "AIä½œæˆ˜æŒ‡æŒ¥å®¤", start_date, end_date)
    candidate_data = fetch_data_for_period(notion, config["CANDIDATE_DB_ID"], "AIå€™é€‰äººåˆ†æä¸­å¿ƒ", start_date, end_date)
    full_period_data = (f"--- æ•°æ®æ¥æº: AIäº’åŠ¨æ—¥å¿— ---\n{log_data}\n\n"
                        f"--- æ•°æ®æ¥æº: AIä½œæˆ˜æŒ‡æŒ¥å®¤ ---\n{brain_data}\n\n"
                        f"--- æ•°æ®æ¥æº: AIå€™é€‰äººåˆ†æä¸­å¿ƒ ---\n{candidate_data}")
                       
    if len(full_period_data.strip()) < 150: 
        print(f"\nğŸŸ¡ è¯¥å‘¨æœŸå†…æ ¸å¿ƒæ•°æ®åº“æ— è¶³å¤Ÿæ•°æ®å¯ä¾›åˆ†æ ({len(full_period_data.strip())} å­—ç¬¦)ï¼Œç¨‹åºé€€å‡ºã€‚")
    else:
        # --- ã€ã€ã€ æ–°å¢çš„Googleæœç´¢æµç¨‹ ã€‘ã€‘ã€‘ ---
        google_search_summary = ""
        if config["ENABLE_GOOGLE_SEARCH"]:
            print_separator()
            print("ğŸŒ æ­£åœ¨å¯åŠ¨å¤–éƒ¨æƒ…æŠ¥æœé›†æ¨¡å—...")
            # ä½¿ç”¨Flashæ¨¡å‹æå–å…³é”®è¯
            queries = extract_search_queries_from_data(gemini_flash_model, full_period_data)
            if queries:
                search_results = [perform_google_search(q, config["GOOGLE_API_KEY"], config["GOOGLE_CSE_ID"]) for q in queries]
                google_search_summary = "\n\n".join(search_results)
            else:
                print("  - æœªèƒ½æå–åˆ°æœ‰ä»·å€¼çš„æœç´¢å…³é”®è¯ï¼Œè·³è¿‡å¤–éƒ¨æƒ…æŠ¥æœé›†ã€‚")
        
        # 3. æ£€ç´¢ç›¸å…³å†å²è®°å¿† (åŸæœ‰é€»è¾‘)
        print_separator()
        query_for_memory = f"ä¸ºæˆ‘çš„{report_type}æŠ¥å‘Šï¼Œæ€»ç»“è¿‡å»çš„æ ¸å¿ƒæˆå°±ã€æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚"
        historical_insights = memory.retrieve_memory(query_for_memory)
        
        # 4. è·å–ä¸“å±Promptå¹¶ç”ŸæˆæŠ¥å‘Š (ä¿®æ”¹ï¼šæ³¨å…¥google_search_summary)
        print_separator()
        prompt = get_prompt_for_report(report_type, full_period_data, start_date_str, end_date_str, historical_insights, google_search_summary)
        
        # 5. è°ƒç”¨AIè¿›è¡Œåˆ†æ (ä¿®æ”¹ï¼šä½¿ç”¨æ›´å¼ºå¤§çš„Proæ¨¡å‹)
        final_report_with_summary = analyze_and_generate_report(gemini_pro_model, prompt)
        
        # 6. ä¿å­˜æŠ¥å‘Šåˆ°Notion (åŸæœ‰é€»è¾‘)
        if final_report_with_summary:
            save_report_to_notion(
                notion, memory, config, report_type, final_report_with_summary, 
                full_period_data, start_date, end_date
            )
            
    print_separator()
    input("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œè¯·æŒ‰å›è½¦é”®é€€å‡ºã€‚")