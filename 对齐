# ==============================================================================
#                 人才分析报告AI (Talent Report AI) v4.3
#                    (Professional Layout Edition)
# ==============================================================================
# 版本说明:
# - 【核心优化】为生成的Word报告正文段落，自动应用“两端对齐”格式，使排版更专业、美观。
# - 【稳定可靠】保留所有v4.2的功能和修复。
# ==============================================================================

import os
import sys
import re
import pandas as pd
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH # 【新增】导入对齐工具
from docx.oxml.ns import qn
from datetime import datetime
import google.generativeai as genai
from notion_client import Client, APIResponseError
from dotenv import load_dotenv
import time
import PyPDF2

# --- 0. 准备工作 ---
def setup_environment():
    print("--- 启动准备工作 (v4.3 专业排版版) ---")
    os.makedirs('output', exist_ok=True)
    os.makedirs('resumes', exist_ok=True)
    print("  - 输出与简历目录已确认。")

# --- 1. 【升级】Word报告生成器 (专业排版版) ---
def generate_word_report(narratives, tables, filename):
    print("✍️ 正在组装Word(.docx)报告 (专业排版)...")
    try:
        doc = Document()
        doc.styles['Normal'].font.name = u'宋体'
        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'宋体')
        
        doc.add_heading(narratives['title'], level=0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        p = doc.add_paragraph(); p.add_run(narratives['subtitle']).italic = True; p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p = doc.add_paragraph(); p.add_run(f"报告生成日期: {datetime.now().strftime('%Y-%m-%d')}").font.size = Pt(9); p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        doc.add_page_break()

        def add_df_to_doc(df, title=""):
            if title: doc.add_paragraph(title, style='Heading 3')
            if df.empty:
                p = doc.add_paragraph("  (无相关数据可生成此表格)"); p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                doc.add_paragraph(); return
            table = doc.add_table(rows=1, cols=len(df.columns)); table.style = 'Table Grid'
            hdr_cells = table.rows[0].cells
            for i, col_name in enumerate(df.columns): hdr_cells[i].text = str(col_name)
            for _, row in df.iterrows():
                row_cells = table.add_row().cells
                for i, value in enumerate(row): row_cells[i].text = str(value)
            doc.add_paragraph()

        for chapter in narratives['chapters']:
            doc.add_heading(chapter['title'], level=1)
            
            # 【核心修复】为所有正文段落应用两端对齐
            content_paragraphs = str(chapter['content']).split('\n')
            for para_text in content_paragraphs:
                if para_text.strip(): # 只为非空行添加段落
                    p = doc.add_paragraph(para_text.strip())
                    p.style = doc.styles['Normal']
                    p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY # <-- 应用两端对齐
            
            # 插入表格的逻辑保持不变
            if chapter['title'] == '内部数据深度洞察':
                add_df_to_doc(tables['status_distribution'], title="表1: 招聘各阶段候选人数量分布")
                add_df_to_doc(tables['skills_distribution'], title="表2: 综合高频技能词统计")
            if chapter['title'] == '必备技能 vs. 高价技能评估':
                add_df_to_doc(tables['must_have_vs_high_value'], title="表3: 核心技能覆盖度评估")
                
            doc.add_paragraph() # 章节末尾增加间距

        doc.save(filename)
        print(f"  - Word报告已成功保存！")
    except Exception as e:
        print(f"❌ 生成Word报告时发生严重错误: {e}")

# --- 其他函数 (初始化, 读取数据, AI分析等) ---
# ... (这部分代码与v4.2完全相同，此处省略以保持简洁)
def initialize():
    print("\n" + "="*70); print("🚀 启动人才分析报告AI引擎 (v4.3)...")
    load_dotenv(); config = {"NOTION_TOKEN": os.getenv("NOTION_API_KEY"), "API_KEY": os.getenv("GEMINI_API_KEY"), "CANDIDATE_DB_ID": os.getenv("LLM_CANDIDATE_DB_ID")}
    if not all(config.values()): print("❌ 致命错误：关键配置缺失！"); sys.exit(1)
    try:
        notion = Client(auth=config["NOTION_TOKEN"]); genai.configure(api_key=config["API_KEY"])
        gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest'); print("✅ 初始化成功！"); return notion, gemini_model, config
    except Exception as e:
        print(f"❌ 初始化失败: {e}"); sys.exit(1)

def read_resumes_from_folder(folder_path):
    absolute_folder_path = os.path.abspath(folder_path); print(f"📂 正在从以下路径读取简历: '{absolute_folder_path}'")
    if not os.path.isdir(absolute_folder_path): print(f"  - ❌ 错误: 简历文件夹路径不存在！"); return ""
    files_in_folder = os.listdir(absolute_folder_path)
    if not files_in_folder: print("  - 🟡 警告: 简历文件夹是空的。"); return ""
    resume_contents = []
    for filename in files_in_folder:
        file_path = os.path.join(absolute_folder_path, filename); content = f"\n--- 简历来源: {filename} ---\n"
        try:
            if filename.lower().endswith('.pdf'):
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    if reader.is_encrypted: print(f"  - 跳过加密的PDF: {filename}"); continue
                    for page in reader.pages: content += page.extract_text() or ""
            elif filename.lower().endswith('.docx'):
                doc = Document(file_path);
                for para in doc.paragraphs: content += para.text + '\n'
            elif filename.lower().endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: content += f.read()
            else: continue
            resume_contents.append(content)
        except Exception as e: print(f"  - ❌ 读取文件 {filename} 时出错: {e}")
    print(f"  - 成功读取 {len(resume_contents)} 份简历。")
    return "\n".join(resume_contents)

def fetch_notion_data(notion, db_id):
    print("\n" + "="*70); print("⏳ 正在从Notion拉取核心数据...")
    all_pages, has_more, start_cursor = [], True, None
    while has_more:
        try:
            response = notion.databases.query(database_id=db_id, start_cursor=start_cursor, page_size=100)
            all_pages.extend(response.get("results", [])); has_more = response.get("has_more", False); start_cursor = response.get("next_cursor")
        except APIResponseError as e:
            if "Could not find database" in str(e): print(f"❌ 查询Notion出错: 找不到数据库。")
            elif "is not shared with the integration" in str(e): print(f"❌ 查询Notion出错: 数据库未分享给机器人。")
            else: print(f"❌ 查询Notion时出错: {e}")
            return pd.DataFrame()
    if not all_pages: print("🟡 未发现Notion候选人数据。"); return pd.DataFrame()
    all_candidates = []
    for page in all_pages:
        props = page.get("properties", {})
        def get_prop(prop_name, prop_type):
            data = props.get(prop_name)
            if not data: return None
            if prop_type == 'title': return data.get('title', [{}])[0].get('plain_text')
            if prop_type == 'rich_text': return ''.join(p.get('plain_text', '') for p in data.get('rich_text', []))
            if prop_type == 'multi_select': return [s['name'] for s in data.get('multi_select', [])]
            if prop_type == 'number': return data.get('number')
            return None
        all_candidates.append({'name': get_prop("候选人姓名", "title"), 'status': get_prop("招聘状态", "multi_select"),'feedback': get_prop("面试官反馈", "rich_text"),'rating': get_prop("匹配度评分", "number"),'reason': get_prop("评分理由", "rich_text")})
    df = pd.DataFrame(all_candidates).dropna(subset=['name'])
    print(f"✅ Notion数据拉取完成！共处理 {len(df)} 条记录。"); return df

def ask_gemini(gemini_model, prompt):
    try:
        response = gemini_model.generate_content(prompt, request_options={"timeout": 180})
        clean_text = re.sub(r'[\*#`]', '', response.text)
        return clean_text
    except Exception as e: print(f"  - 警告: 调用AI失败: {e}."); return "AI未能生成解读内容。"

def process_data_and_generate_narratives(gemini_model, jd_text, df_notion, resumes_text):
    print("\n" + "="*70); print("🔄 正在进行深度量化分析与AI解读...")
    all_text = ' '.join(df_notion['feedback'].dropna().astype(str)) + ' '.join(df_notion['reason'].dropna().astype(str)) + resumes_text
    must_have_skills = ['Python', 'PyTorch', 'Transformer', 'NLP']
    high_value_skills = ['DeepSpeed', 'RLHF', 'CUDA', '昇腾', '分布式', '量化', '多模态', 'AI Agent']
    tables = {}
    if 'status' in df_notion.columns and not df_notion['status'].dropna().empty:
        status_flat = [s for sublist in df_notion['status'].dropna() for s in sublist]
        tables['status_distribution'] = pd.DataFrame(pd.Series(status_flat).value_counts()).reset_index().rename(columns={'index': '招聘状态', 0: '候选人数量'})
    skill_counts = {skill: all_text.lower().count(skill.lower()) for skill in must_have_skills + high_value_skills}
    tables['skills_distribution'] = pd.DataFrame(list(skill_counts.items()), columns=['技能', '提及次数']).sort_values(by='提及次数', ascending=False)
    must_have_coverage = {skill: skill_counts[skill] for skill in must_have_skills}
    high_value_coverage = {skill: skill_counts[skill] for skill in high_value_skills}
    tables['must_have_vs_high_value'] = pd.DataFrame([
        {'技能类型': '必备技能', '技能列表': ', '.join(must_have_skills), '总提及次数': sum(must_have_coverage.values())},
        {'技能类型': '高价技能', '技能列表': ', '.join(high_value_skills), '总提及次数': sum(high_value_coverage.values())}
    ])
    print("✅ 量化分析表格已生成！")
    print("🧠 正在调用AI解读表格...")
    narratives = {"title": "AI大模型工程师人才分析报告", "subtitle": "基于内部数据的深度量化洞察", "chapters": []}
    chapters_prompts = [
        {"title": "执行摘要", "prompt": f"作为AI人才战略顾问，请基于以下所有表格数据，撰写一份高度概括的执行摘要。\n\n招聘漏斗数据:\n{tables.get('status_distribution', pd.DataFrame()).to_markdown()}\n\n技能分布数据:\n{tables.get('skills_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "人才市场宏观分析", "prompt": "作为行业分析师，请单纯基于你的知识库，分析当前AI大模型算法工程师市场的供需情况，并详细阐述“必备技能”和“高价技能”之间的区别和价值。"},
        {"title": "招聘漏斗与转化分析", "prompt": f"请解读下面的“招聘各阶段候选人数量分布”表格。分析我们的招聘漏斗形态是否健康？\n\n{tables.get('status_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "核心技能图谱量化分析", "prompt": f"请解读下面的“综合高频技能词统计”表格。基于这些数据，我们当前人才池的整体技术画像是怎样的？\n\n{tables.get('skills_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "必备技能 vs. 高价技能评估", "prompt": f"这张表格对比了我们定义的两类技能的覆盖情况。请深入分析这揭示了我们招聘策略的哪些问题？\n\n{tables.get('must_have_vs_high_value', pd.DataFrame()).to_markdown()}"},
        {"title": "战略行动建议", "prompt": f"综合以上所有表格和你的解读，请为我们的招聘团队提出3-5条具体的战略建议。"}
    ]
    for chapter in chapters_prompts:
        print(f"  - 正在生成章节: [{chapter['title']}]...")
        content = ask_gemini(gemini_model, chapter['prompt'])
        narratives['chapters'].append({"title": chapter['title'], "content": content})
    print("✅ 所有报告章节已由AI生成！")
    return narratives, tables

# --- 主程序入口 ---
if __name__ == "__main__":
    setup_environment()
    notion, gemini_model, config = initialize()
    df_notion_data = fetch_notion_data(notion, config["CANDIDATE_DB_ID"])
    resumes_text = read_resumes_from_folder('resumes')
    if df_notion_data.empty and not resumes_text:
        print("\n❌ 致命错误: 无数据可供分析。")
    else:
        try:
            with open('jd_template.txt', 'r', encoding='utf-8') as f:
                jd_text = f.read()
        except FileNotFoundError:
            print("❌ 错误: 'jd_template.txt' 文件未找到。"); sys.exit(1)
        narratives, tables = process_data_and_generate_narratives(gemini_model, jd_text, df_notion_data, resumes_text)
        if narratives:
            base_filename = f"output/Talent_Analysis_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            docx_filename = f"{base_filename}.docx"
            generate_word_report(narratives, tables, docx_filename)
            print(f"\n🎉🎉🎉 恭喜！专业排版报告已生成:")
            print(f"   - Word版 (包含文字与表格): {docx_filename}")
    print("\n" + "="*70); print("所有任务执行完毕...")