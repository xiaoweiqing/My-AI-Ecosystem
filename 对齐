# ==============================================================================
#                 äººæ‰åˆ†ææŠ¥å‘ŠAI (Talent Report AI) v4.3
#                    (Professional Layout Edition)
# ==============================================================================
# ç‰ˆæœ¬è¯´æ˜:
# - ã€æ ¸å¿ƒä¼˜åŒ–ã€‘ä¸ºç”Ÿæˆçš„WordæŠ¥å‘Šæ­£æ–‡æ®µè½ï¼Œè‡ªåŠ¨åº”ç”¨â€œä¸¤ç«¯å¯¹é½â€æ ¼å¼ï¼Œä½¿æ’ç‰ˆæ›´ä¸“ä¸šã€ç¾è§‚ã€‚
# - ã€ç¨³å®šå¯é ã€‘ä¿ç•™æ‰€æœ‰v4.2çš„åŠŸèƒ½å’Œä¿®å¤ã€‚
# ==============================================================================

import os
import sys
import re
import pandas as pd
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH # ã€æ–°å¢ã€‘å¯¼å…¥å¯¹é½å·¥å…·
from docx.oxml.ns import qn
from datetime import datetime
import google.generativeai as genai
from notion_client import Client, APIResponseError
from dotenv import load_dotenv
import time
import PyPDF2

# --- 0. å‡†å¤‡å·¥ä½œ ---
def setup_environment():
    print("--- å¯åŠ¨å‡†å¤‡å·¥ä½œ (v4.3 ä¸“ä¸šæ’ç‰ˆç‰ˆ) ---")
    os.makedirs('output', exist_ok=True)
    os.makedirs('resumes', exist_ok=True)
    print("  - è¾“å‡ºä¸ç®€å†ç›®å½•å·²ç¡®è®¤ã€‚")

# --- 1. ã€å‡çº§ã€‘WordæŠ¥å‘Šç”Ÿæˆå™¨ (ä¸“ä¸šæ’ç‰ˆç‰ˆ) ---
def generate_word_report(narratives, tables, filename):
    print("âœï¸ æ­£åœ¨ç»„è£…Word(.docx)æŠ¥å‘Š (ä¸“ä¸šæ’ç‰ˆ)...")
    try:
        doc = Document()
        doc.styles['Normal'].font.name = u'å®‹ä½“'
        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'å®‹ä½“')
        
        doc.add_heading(narratives['title'], level=0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        p = doc.add_paragraph(); p.add_run(narratives['subtitle']).italic = True; p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p = doc.add_paragraph(); p.add_run(f"æŠ¥å‘Šç”Ÿæˆæ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}").font.size = Pt(9); p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        doc.add_page_break()

        def add_df_to_doc(df, title=""):
            if title: doc.add_paragraph(title, style='Heading 3')
            if df.empty:
                p = doc.add_paragraph("  (æ— ç›¸å…³æ•°æ®å¯ç”Ÿæˆæ­¤è¡¨æ ¼)"); p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                doc.add_paragraph(); return
            table = doc.add_table(rows=1, cols=len(df.columns)); table.style = 'Table Grid'
            hdr_cells = table.rows[0].cells
            for i, col_name in enumerate(df.columns): hdr_cells[i].text = str(col_name)
            for _, row in df.iterrows():
                row_cells = table.add_row().cells
                for i, value in enumerate(row): row_cells[i].text = str(value)
            doc.add_paragraph()

        for chapter in narratives['chapters']:
            doc.add_heading(chapter['title'], level=1)
            
            # ã€æ ¸å¿ƒä¿®å¤ã€‘ä¸ºæ‰€æœ‰æ­£æ–‡æ®µè½åº”ç”¨ä¸¤ç«¯å¯¹é½
            content_paragraphs = str(chapter['content']).split('\n')
            for para_text in content_paragraphs:
                if para_text.strip(): # åªä¸ºéç©ºè¡Œæ·»åŠ æ®µè½
                    p = doc.add_paragraph(para_text.strip())
                    p.style = doc.styles['Normal']
                    p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY # <-- åº”ç”¨ä¸¤ç«¯å¯¹é½
            
            # æ’å…¥è¡¨æ ¼çš„é€»è¾‘ä¿æŒä¸å˜
            if chapter['title'] == 'å†…éƒ¨æ•°æ®æ·±åº¦æ´å¯Ÿ':
                add_df_to_doc(tables['status_distribution'], title="è¡¨1: æ‹›è˜å„é˜¶æ®µå€™é€‰äººæ•°é‡åˆ†å¸ƒ")
                add_df_to_doc(tables['skills_distribution'], title="è¡¨2: ç»¼åˆé«˜é¢‘æŠ€èƒ½è¯ç»Ÿè®¡")
            if chapter['title'] == 'å¿…å¤‡æŠ€èƒ½ vs. é«˜ä»·æŠ€èƒ½è¯„ä¼°':
                add_df_to_doc(tables['must_have_vs_high_value'], title="è¡¨3: æ ¸å¿ƒæŠ€èƒ½è¦†ç›–åº¦è¯„ä¼°")
                
            doc.add_paragraph() # ç« èŠ‚æœ«å°¾å¢åŠ é—´è·

        doc.save(filename)
        print(f"  - WordæŠ¥å‘Šå·²æˆåŠŸä¿å­˜ï¼")
    except Exception as e:
        print(f"âŒ ç”ŸæˆWordæŠ¥å‘Šæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

# --- å…¶ä»–å‡½æ•° (åˆå§‹åŒ–, è¯»å–æ•°æ®, AIåˆ†æç­‰) ---
# ... (è¿™éƒ¨åˆ†ä»£ç ä¸v4.2å®Œå…¨ç›¸åŒï¼Œæ­¤å¤„çœç•¥ä»¥ä¿æŒç®€æ´)
def initialize():
    print("\n" + "="*70); print("ğŸš€ å¯åŠ¨äººæ‰åˆ†ææŠ¥å‘ŠAIå¼•æ“ (v4.3)...")
    load_dotenv(); config = {"NOTION_TOKEN": os.getenv("NOTION_API_KEY"), "API_KEY": os.getenv("GEMINI_API_KEY"), "CANDIDATE_DB_ID": os.getenv("LLM_CANDIDATE_DB_ID")}
    if not all(config.values()): print("âŒ è‡´å‘½é”™è¯¯ï¼šå…³é”®é…ç½®ç¼ºå¤±ï¼"); sys.exit(1)
    try:
        notion = Client(auth=config["NOTION_TOKEN"]); genai.configure(api_key=config["API_KEY"])
        gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest'); print("âœ… åˆå§‹åŒ–æˆåŠŸï¼"); return notion, gemini_model, config
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"); sys.exit(1)

def read_resumes_from_folder(folder_path):
    absolute_folder_path = os.path.abspath(folder_path); print(f"ğŸ“‚ æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„è¯»å–ç®€å†: '{absolute_folder_path}'")
    if not os.path.isdir(absolute_folder_path): print(f"  - âŒ é”™è¯¯: ç®€å†æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨ï¼"); return ""
    files_in_folder = os.listdir(absolute_folder_path)
    if not files_in_folder: print("  - ğŸŸ¡ è­¦å‘Š: ç®€å†æ–‡ä»¶å¤¹æ˜¯ç©ºçš„ã€‚"); return ""
    resume_contents = []
    for filename in files_in_folder:
        file_path = os.path.join(absolute_folder_path, filename); content = f"\n--- ç®€å†æ¥æº: {filename} ---\n"
        try:
            if filename.lower().endswith('.pdf'):
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    if reader.is_encrypted: print(f"  - è·³è¿‡åŠ å¯†çš„PDF: {filename}"); continue
                    for page in reader.pages: content += page.extract_text() or ""
            elif filename.lower().endswith('.docx'):
                doc = Document(file_path);
                for para in doc.paragraphs: content += para.text + '\n'
            elif filename.lower().endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: content += f.read()
            else: continue
            resume_contents.append(content)
        except Exception as e: print(f"  - âŒ è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
    print(f"  - æˆåŠŸè¯»å– {len(resume_contents)} ä»½ç®€å†ã€‚")
    return "\n".join(resume_contents)

def fetch_notion_data(notion, db_id):
    print("\n" + "="*70); print("â³ æ­£åœ¨ä»Notionæ‹‰å–æ ¸å¿ƒæ•°æ®...")
    all_pages, has_more, start_cursor = [], True, None
    while has_more:
        try:
            response = notion.databases.query(database_id=db_id, start_cursor=start_cursor, page_size=100)
            all_pages.extend(response.get("results", [])); has_more = response.get("has_more", False); start_cursor = response.get("next_cursor")
        except APIResponseError as e:
            if "Could not find database" in str(e): print(f"âŒ æŸ¥è¯¢Notionå‡ºé”™: æ‰¾ä¸åˆ°æ•°æ®åº“ã€‚")
            elif "is not shared with the integration" in str(e): print(f"âŒ æŸ¥è¯¢Notionå‡ºé”™: æ•°æ®åº“æœªåˆ†äº«ç»™æœºå™¨äººã€‚")
            else: print(f"âŒ æŸ¥è¯¢Notionæ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
    if not all_pages: print("ğŸŸ¡ æœªå‘ç°Notionå€™é€‰äººæ•°æ®ã€‚"); return pd.DataFrame()
    all_candidates = []
    for page in all_pages:
        props = page.get("properties", {})
        def get_prop(prop_name, prop_type):
            data = props.get(prop_name)
            if not data: return None
            if prop_type == 'title': return data.get('title', [{}])[0].get('plain_text')
            if prop_type == 'rich_text': return ''.join(p.get('plain_text', '') for p in data.get('rich_text', []))
            if prop_type == 'multi_select': return [s['name'] for s in data.get('multi_select', [])]
            if prop_type == 'number': return data.get('number')
            return None
        all_candidates.append({'name': get_prop("å€™é€‰äººå§“å", "title"), 'status': get_prop("æ‹›è˜çŠ¶æ€", "multi_select"),'feedback': get_prop("é¢è¯•å®˜åé¦ˆ", "rich_text"),'rating': get_prop("åŒ¹é…åº¦è¯„åˆ†", "number"),'reason': get_prop("è¯„åˆ†ç†ç”±", "rich_text")})
    df = pd.DataFrame(all_candidates).dropna(subset=['name'])
    print(f"âœ… Notionæ•°æ®æ‹‰å–å®Œæˆï¼å…±å¤„ç† {len(df)} æ¡è®°å½•ã€‚"); return df

def ask_gemini(gemini_model, prompt):
    try:
        response = gemini_model.generate_content(prompt, request_options={"timeout": 180})
        clean_text = re.sub(r'[\*#`]', '', response.text)
        return clean_text
    except Exception as e: print(f"  - è­¦å‘Š: è°ƒç”¨AIå¤±è´¥: {e}."); return "AIæœªèƒ½ç”Ÿæˆè§£è¯»å†…å®¹ã€‚"

def process_data_and_generate_narratives(gemini_model, jd_text, df_notion, resumes_text):
    print("\n" + "="*70); print("ğŸ”„ æ­£åœ¨è¿›è¡Œæ·±åº¦é‡åŒ–åˆ†æä¸AIè§£è¯»...")
    all_text = ' '.join(df_notion['feedback'].dropna().astype(str)) + ' '.join(df_notion['reason'].dropna().astype(str)) + resumes_text
    must_have_skills = ['Python', 'PyTorch', 'Transformer', 'NLP']
    high_value_skills = ['DeepSpeed', 'RLHF', 'CUDA', 'æ˜‡è…¾', 'åˆ†å¸ƒå¼', 'é‡åŒ–', 'å¤šæ¨¡æ€', 'AI Agent']
    tables = {}
    if 'status' in df_notion.columns and not df_notion['status'].dropna().empty:
        status_flat = [s for sublist in df_notion['status'].dropna() for s in sublist]
        tables['status_distribution'] = pd.DataFrame(pd.Series(status_flat).value_counts()).reset_index().rename(columns={'index': 'æ‹›è˜çŠ¶æ€', 0: 'å€™é€‰äººæ•°é‡'})
    skill_counts = {skill: all_text.lower().count(skill.lower()) for skill in must_have_skills + high_value_skills}
    tables['skills_distribution'] = pd.DataFrame(list(skill_counts.items()), columns=['æŠ€èƒ½', 'æåŠæ¬¡æ•°']).sort_values(by='æåŠæ¬¡æ•°', ascending=False)
    must_have_coverage = {skill: skill_counts[skill] for skill in must_have_skills}
    high_value_coverage = {skill: skill_counts[skill] for skill in high_value_skills}
    tables['must_have_vs_high_value'] = pd.DataFrame([
        {'æŠ€èƒ½ç±»å‹': 'å¿…å¤‡æŠ€èƒ½', 'æŠ€èƒ½åˆ—è¡¨': ', '.join(must_have_skills), 'æ€»æåŠæ¬¡æ•°': sum(must_have_coverage.values())},
        {'æŠ€èƒ½ç±»å‹': 'é«˜ä»·æŠ€èƒ½', 'æŠ€èƒ½åˆ—è¡¨': ', '.join(high_value_skills), 'æ€»æåŠæ¬¡æ•°': sum(high_value_coverage.values())}
    ])
    print("âœ… é‡åŒ–åˆ†æè¡¨æ ¼å·²ç”Ÿæˆï¼")
    print("ğŸ§  æ­£åœ¨è°ƒç”¨AIè§£è¯»è¡¨æ ¼...")
    narratives = {"title": "AIå¤§æ¨¡å‹å·¥ç¨‹å¸ˆäººæ‰åˆ†ææŠ¥å‘Š", "subtitle": "åŸºäºå†…éƒ¨æ•°æ®çš„æ·±åº¦é‡åŒ–æ´å¯Ÿ", "chapters": []}
    chapters_prompts = [
        {"title": "æ‰§è¡Œæ‘˜è¦", "prompt": f"ä½œä¸ºAIäººæ‰æˆ˜ç•¥é¡¾é—®ï¼Œè¯·åŸºäºä»¥ä¸‹æ‰€æœ‰è¡¨æ ¼æ•°æ®ï¼Œæ’°å†™ä¸€ä»½é«˜åº¦æ¦‚æ‹¬çš„æ‰§è¡Œæ‘˜è¦ã€‚\n\næ‹›è˜æ¼æ–—æ•°æ®:\n{tables.get('status_distribution', pd.DataFrame()).to_markdown()}\n\næŠ€èƒ½åˆ†å¸ƒæ•°æ®:\n{tables.get('skills_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "äººæ‰å¸‚åœºå®è§‚åˆ†æ", "prompt": "ä½œä¸ºè¡Œä¸šåˆ†æå¸ˆï¼Œè¯·å•çº¯åŸºäºä½ çš„çŸ¥è¯†åº“ï¼Œåˆ†æå½“å‰AIå¤§æ¨¡å‹ç®—æ³•å·¥ç¨‹å¸ˆå¸‚åœºçš„ä¾›éœ€æƒ…å†µï¼Œå¹¶è¯¦ç»†é˜è¿°â€œå¿…å¤‡æŠ€èƒ½â€å’Œâ€œé«˜ä»·æŠ€èƒ½â€ä¹‹é—´çš„åŒºåˆ«å’Œä»·å€¼ã€‚"},
        {"title": "æ‹›è˜æ¼æ–—ä¸è½¬åŒ–åˆ†æ", "prompt": f"è¯·è§£è¯»ä¸‹é¢çš„â€œæ‹›è˜å„é˜¶æ®µå€™é€‰äººæ•°é‡åˆ†å¸ƒâ€è¡¨æ ¼ã€‚åˆ†ææˆ‘ä»¬çš„æ‹›è˜æ¼æ–—å½¢æ€æ˜¯å¦å¥åº·ï¼Ÿ\n\n{tables.get('status_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "æ ¸å¿ƒæŠ€èƒ½å›¾è°±é‡åŒ–åˆ†æ", "prompt": f"è¯·è§£è¯»ä¸‹é¢çš„â€œç»¼åˆé«˜é¢‘æŠ€èƒ½è¯ç»Ÿè®¡â€è¡¨æ ¼ã€‚åŸºäºè¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å½“å‰äººæ‰æ± çš„æ•´ä½“æŠ€æœ¯ç”»åƒæ˜¯æ€æ ·çš„ï¼Ÿ\n\n{tables.get('skills_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "å¿…å¤‡æŠ€èƒ½ vs. é«˜ä»·æŠ€èƒ½è¯„ä¼°", "prompt": f"è¿™å¼ è¡¨æ ¼å¯¹æ¯”äº†æˆ‘ä»¬å®šä¹‰çš„ä¸¤ç±»æŠ€èƒ½çš„è¦†ç›–æƒ…å†µã€‚è¯·æ·±å…¥åˆ†æè¿™æ­ç¤ºäº†æˆ‘ä»¬æ‹›è˜ç­–ç•¥çš„å“ªäº›é—®é¢˜ï¼Ÿ\n\n{tables.get('must_have_vs_high_value', pd.DataFrame()).to_markdown()}"},
        {"title": "æˆ˜ç•¥è¡ŒåŠ¨å»ºè®®", "prompt": f"ç»¼åˆä»¥ä¸Šæ‰€æœ‰è¡¨æ ¼å’Œä½ çš„è§£è¯»ï¼Œè¯·ä¸ºæˆ‘ä»¬çš„æ‹›è˜å›¢é˜Ÿæå‡º3-5æ¡å…·ä½“çš„æˆ˜ç•¥å»ºè®®ã€‚"}
    ]
    for chapter in chapters_prompts:
        print(f"  - æ­£åœ¨ç”Ÿæˆç« èŠ‚: [{chapter['title']}]...")
        content = ask_gemini(gemini_model, chapter['prompt'])
        narratives['chapters'].append({"title": chapter['title'], "content": content})
    print("âœ… æ‰€æœ‰æŠ¥å‘Šç« èŠ‚å·²ç”±AIç”Ÿæˆï¼")
    return narratives, tables

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    setup_environment()
    notion, gemini_model, config = initialize()
    df_notion_data = fetch_notion_data(notion, config["CANDIDATE_DB_ID"])
    resumes_text = read_resumes_from_folder('resumes')
    if df_notion_data.empty and not resumes_text:
        print("\nâŒ è‡´å‘½é”™è¯¯: æ— æ•°æ®å¯ä¾›åˆ†æã€‚")
    else:
        try:
            with open('jd_template.txt', 'r', encoding='utf-8') as f:
                jd_text = f.read()
        except FileNotFoundError:
            print("âŒ é”™è¯¯: 'jd_template.txt' æ–‡ä»¶æœªæ‰¾åˆ°ã€‚"); sys.exit(1)
        narratives, tables = process_data_and_generate_narratives(gemini_model, jd_text, df_notion_data, resumes_text)
        if narratives:
            base_filename = f"output/Talent_Analysis_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            docx_filename = f"{base_filename}.docx"
            generate_word_report(narratives, tables, docx_filename)
            print(f"\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼ä¸“ä¸šæ’ç‰ˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
            print(f"   - Wordç‰ˆ (åŒ…å«æ–‡å­—ä¸è¡¨æ ¼): {docx_filename}")
    print("\n" + "="*70); print("æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•...")