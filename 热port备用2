# ==============================================================================
#                 äººæ‰åˆ†ææŠ¥å‘ŠAI (Talent Report AI) v4.2
#                    (BI & Quant Edition)
# ==============================================================================
# ç‰ˆæœ¬è¯´æ˜:
# - ã€æ ¸å¿ƒå‡çº§ã€‘ç”Ÿæˆå¤šä»½æ·±åº¦é‡åŒ–åˆ†æè¡¨æ ¼ï¼Œæ¨¡ä»¿ä¸“ä¸šBIæŠ¥å‘Šã€‚
# - ã€AIè§’è‰²è½¬å˜ã€‘AIä¸å†è¿›è¡Œåˆçº§åˆ†æï¼Œè€Œæ˜¯æ‰®æ¼”è§£è¯»å·²ç”Ÿæˆè¡¨æ ¼çš„æ•°æ®åˆ†æå¸ˆè§’è‰²ã€‚
# - ã€æ ¼å¼ä¼˜åŒ–ã€‘ç¡®ä¿è¾“å‡ºçš„Wordæ–‡æ¡£å¹²å‡€ã€ä¸“ä¸šï¼Œæ— å¤šä½™ç¬¦å·ã€‚
# - ã€ç¨³å®šå¯é ã€‘ä¿ç•™æ‰€æœ‰ä¹‹å‰çš„bugä¿®å¤å’Œå®šåˆ¶åŒ–é€‚é…ã€‚
# ==============================================================================

import os
import sys
import re
import pandas as pd
from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn
from datetime import datetime
import google.generativeai as genai
from notion_client import Client, APIResponseError
from dotenv import load_dotenv
import time
import PyPDF2

# --- 0. å‡†å¤‡å·¥ä½œ ---
def setup_environment():
    print("--- å¯åŠ¨å‡†å¤‡å·¥ä½œ (v4.2 BIé‡åŒ–ç‰ˆ) ---")
    os.makedirs('output', exist_ok=True)
    os.makedirs('resumes', exist_ok=True)
    print("  - è¾“å‡ºä¸ç®€å†ç›®å½•å·²ç¡®è®¤ã€‚")

# --- 1. ã€å‡çº§ã€‘WordæŠ¥å‘Šç”Ÿæˆå™¨ (æ”¯æŒå¤šè¡¨æ ¼) ---
def generate_word_report(narratives, tables, filename):
    print("âœï¸ æ­£åœ¨ç»„è£…Word(.docx)æŠ¥å‘Š (BIç‰ˆ)...")
    try:
        doc = Document()
        doc.styles['Normal'].font.name = u'å®‹ä½“'
        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'å®‹ä½“')
        
        doc.add_heading(narratives['title'], level=0)
        p = doc.add_paragraph(); p.add_run(narratives['subtitle']).italic = True
        p = doc.add_paragraph(); p.add_run(f"æŠ¥å‘Šç”Ÿæˆæ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}").font.size = Pt(9)
        doc.add_page_break()

        def add_df_to_doc(df, title=""):
            if title: doc.add_paragraph(title, style='Heading 3')
            if df.empty:
                doc.add_paragraph("  (æ— ç›¸å…³æ•°æ®å¯ç”Ÿæˆæ­¤è¡¨æ ¼)")
                doc.add_paragraph(); return
            table = doc.add_table(rows=1, cols=len(df.columns)); table.style = 'Table Grid'
            hdr_cells = table.rows[0].cells
            for i, col_name in enumerate(df.columns): hdr_cells[i].text = str(col_name)
            for _, row in df.iterrows():
                row_cells = table.add_row().cells
                for i, value in enumerate(row): row_cells[i].text = str(value)
            doc.add_paragraph()

        # å¾ªç¯ç”Ÿæˆç« èŠ‚å’Œè¡¨æ ¼
        for chapter in narratives['chapters']:
            doc.add_heading(chapter['title'], level=1)
            p = doc.add_paragraph(str(chapter['content'])); p.style = doc.styles['Normal']
            
            # æ ¹æ®ç« èŠ‚æ ‡é¢˜ï¼Œæ’å…¥å¯¹åº”çš„è¡¨æ ¼
            if chapter['title'] == 'æ‹›è˜æ¼æ–—ä¸è½¬åŒ–åˆ†æ' and 'status_distribution' in tables:
                add_df_to_doc(tables['status_distribution'], title="è¡¨1: æ‹›è˜å„é˜¶æ®µå€™é€‰äººæ•°é‡åˆ†å¸ƒ")
            
            if chapter['title'] == 'æ ¸å¿ƒæŠ€èƒ½å›¾è°±é‡åŒ–åˆ†æ' and 'skills_distribution' in tables:
                add_df_to_doc(tables['skills_distribution'], title="è¡¨2: ç»¼åˆé«˜é¢‘æŠ€èƒ½è¯ç»Ÿè®¡")
            
            if chapter['title'] == 'å¿…å¤‡æŠ€èƒ½ vs. é«˜ä»·æŠ€èƒ½è¯„ä¼°' and 'must_have_vs_high_value' in tables:
                add_df_to_doc(tables['must_have_vs_high_value'], title="è¡¨3: æ ¸å¿ƒæŠ€èƒ½è¦†ç›–åº¦è¯„ä¼°")
                
            doc.add_paragraph()

        doc.save(filename)
        print(f"  - WordæŠ¥å‘Šå·²æˆåŠŸä¿å­˜ï¼")
    except Exception as e:
        print(f"âŒ ç”ŸæˆWordæŠ¥å‘Šæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

# --- 2. åˆå§‹åŒ–ä¸é…ç½®åŠ è½½ (ä¿æŒä¸å˜) ---
def initialize():
    print("\n" + "="*70); print("ğŸš€ å¯åŠ¨äººæ‰åˆ†ææŠ¥å‘ŠAIå¼•æ“ (v4.2)...")
    load_dotenv(); config = {"NOTION_TOKEN": os.getenv("NOTION_API_KEY"), "API_KEY": os.getenv("GEMINI_API_KEY"), "CANDIDATE_DB_ID": os.getenv("LLM_CANDIDATE_DB_ID")}
    if not all(config.values()):
        print("âŒ è‡´å‘½é”™è¯¯ï¼šå…³é”®é…ç½®ç¼ºå¤±ï¼"); sys.exit(1)
    try:
        notion = Client(auth=config["NOTION_TOKEN"]); genai.configure(api_key=config["API_KEY"])
        gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest'); print("âœ… åˆå§‹åŒ–æˆåŠŸï¼"); return notion, gemini_model, config
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"); sys.exit(1)

# --- 3. æ•°æ®è¯»å–æ¨¡å— (ä¸v4.1ç›¸åŒ) ---
def read_resumes_from_folder(folder_path):
    absolute_folder_path = os.path.abspath(folder_path); print(f"ğŸ“‚ æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„è¯»å–ç®€å†: '{absolute_folder_path}'")
    if not os.path.isdir(absolute_folder_path): print(f"  - âŒ é”™è¯¯: ç®€å†æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨ï¼"); return ""
    files_in_folder = os.listdir(absolute_folder_path)
    if not files_in_folder: print("  - ğŸŸ¡ è­¦å‘Š: ç®€å†æ–‡ä»¶å¤¹æ˜¯ç©ºçš„ã€‚"); return ""
    resume_contents = []
    for filename in files_in_folder:
        file_path = os.path.join(absolute_folder_path, filename); content = f"\n--- ç®€å†æ¥æº: {filename} ---\n"
        try:
            if filename.lower().endswith('.pdf'):
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    if reader.is_encrypted: print(f"  - è·³è¿‡åŠ å¯†çš„PDF: {filename}"); continue
                    for page in reader.pages: content += page.extract_text() or ""
            elif filename.lower().endswith('.docx'):
                doc = Document(file_path);
                for para in doc.paragraphs: content += para.text + '\n'
            elif filename.lower().endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: content += f.read()
            else: continue
            resume_contents.append(content)
        except Exception as e: print(f"  - âŒ è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
    print(f"  - æˆåŠŸè¯»å– {len(resume_contents)} ä»½ç®€å†ã€‚")
    return "\n".join(resume_contents)

def fetch_notion_data(notion, db_id):
    print("\n" + "="*70); print("â³ æ­£åœ¨ä»Notionæ‹‰å–æ ¸å¿ƒæ•°æ®...")
    all_pages, has_more, start_cursor = [], True, None
    while has_more:
        try:
            response = notion.databases.query(database_id=db_id, start_cursor=start_cursor, page_size=100)
            all_pages.extend(response.get("results", [])); has_more = response.get("has_more", False); start_cursor = response.get("next_cursor")
        except APIResponseError as e:
            if "Could not find database" in str(e): print(f"âŒ æŸ¥è¯¢Notionå‡ºé”™: æ‰¾ä¸åˆ°æ•°æ®åº“ã€‚")
            elif "is not shared with the integration" in str(e): print(f"âŒ æŸ¥è¯¢Notionå‡ºé”™: æ•°æ®åº“æœªåˆ†äº«ç»™æœºå™¨äººã€‚")
            else: print(f"âŒ æŸ¥è¯¢Notionæ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
    if not all_pages: print("ğŸŸ¡ æœªå‘ç°Notionå€™é€‰äººæ•°æ®ã€‚"); return pd.DataFrame()
    all_candidates = []
    for page in all_pages:
        props = page.get("properties", {})
        def get_prop(prop_name, prop_type):
            data = props.get(prop_name)
            if not data: return None
            if prop_type == 'title': return data.get('title', [{}])[0].get('plain_text')
            if prop_type == 'rich_text': return ''.join(p.get('plain_text', '') for p in data.get('rich_text', []))
            if prop_type == 'multi_select': return [s['name'] for s in data.get('multi_select', [])]
            if prop_type == 'number': return data.get('number')
            return None
        all_candidates.append({'name': get_prop("å€™é€‰äººå§“å", "title"), 'status': get_prop("æ‹›è˜çŠ¶æ€", "multi_select"),'feedback': get_prop("é¢è¯•å®˜åé¦ˆ", "rich_text"),'rating': get_prop("åŒ¹é…åº¦è¯„åˆ†", "number"),'reason': get_prop("è¯„åˆ†ç†ç”±", "rich_text")})
    df = pd.DataFrame(all_candidates).dropna(subset=['name'])
    print(f"âœ… Notionæ•°æ®æ‹‰å–å®Œæˆï¼å…±å¤„ç† {len(df)} æ¡è®°å½•ã€‚"); return df

# --- 4. ã€å‡çº§ã€‘AIåˆ†æä¸æ•°æ®å¤„ç†æ¨¡å— ---
def ask_gemini(gemini_model, prompt):
    try:
        response = gemini_model.generate_content(prompt, request_options={"timeout": 180})
        # æ¸…ç†AIè¿”å›çš„Markdownç¬¦å·
        clean_text = re.sub(r'[\*#`]', '', response.text)
        return clean_text
    except Exception as e: print(f"  - è­¦å‘Š: è°ƒç”¨AIå¤±è´¥: {e}."); return "AIæœªèƒ½ç”Ÿæˆè§£è¯»å†…å®¹ã€‚"

def process_data_and_generate_narratives(gemini_model, jd_text, df_notion, resumes_text):
    print("\n" + "="*70); print("ğŸ”„ æ­£åœ¨è¿›è¡Œæ·±åº¦é‡åŒ–åˆ†æä¸AIè§£è¯»...")
    
    # --- 1. æ•°æ®å¤„ç†ä¸é‡åŒ–åˆ†æ ---
    all_text = ' '.join(df_notion['feedback'].dropna().astype(str)) + ' '.join(df_notion['reason'].dropna().astype(str)) + resumes_text
    
    # å®šä¹‰æŠ€èƒ½è¯
    must_have_skills = ['Python', 'PyTorch', 'Transformer', 'NLP']
    high_value_skills = ['DeepSpeed', 'RLHF', 'CUDA', 'æ˜‡è…¾', 'åˆ†å¸ƒå¼', 'é‡åŒ–', 'å¤šæ¨¡æ€', 'AI Agent']
    
    # ç”Ÿæˆé‡åŒ–è¡¨æ ¼
    tables = {}
    if 'status' in df_notion.columns and not df_notion['status'].dropna().empty:
        status_flat = [s for sublist in df_notion['status'].dropna() for s in sublist]
        tables['status_distribution'] = pd.DataFrame(pd.Series(status_flat).value_counts()).reset_index().rename(columns={'index': 'æ‹›è˜çŠ¶æ€', 0: 'å€™é€‰äººæ•°é‡'})

    skill_counts = {skill: all_text.lower().count(skill.lower()) for skill in must_have_skills + high_value_skills}
    tables['skills_distribution'] = pd.DataFrame(list(skill_counts.items()), columns=['æŠ€èƒ½', 'æåŠæ¬¡æ•°']).sort_values(by='æåŠæ¬¡æ•°', ascending=False)
    
    must_have_coverage = {skill: skill_counts[skill] for skill in must_have_skills}
    high_value_coverage = {skill: skill_counts[skill] for skill in high_value_skills}
    tables['must_have_vs_high_value'] = pd.DataFrame([
        {'æŠ€èƒ½ç±»å‹': 'å¿…å¤‡æŠ€èƒ½', 'æŠ€èƒ½åˆ—è¡¨': ', '.join(must_have_skills), 'æ€»æåŠæ¬¡æ•°': sum(must_have_coverage.values())},
        {'æŠ€èƒ½ç±»å‹': 'é«˜ä»·æŠ€èƒ½', 'æŠ€èƒ½åˆ—è¡¨': ', '.join(high_value_skills), 'æ€»æåŠæ¬¡æ•°': sum(high_value_coverage.values())}
    ])
    print("âœ… é‡åŒ–åˆ†æè¡¨æ ¼å·²ç”Ÿæˆï¼")

    # --- 2. AIç”Ÿæˆæ–‡å­—è§£è¯» ---
    print("ğŸ§  æ­£åœ¨è°ƒç”¨AIè§£è¯»è¡¨æ ¼...")
    narratives = {"title": "AIå¤§æ¨¡å‹å·¥ç¨‹å¸ˆäººæ‰åˆ†ææŠ¥å‘Š", "subtitle": "åŸºäºå†…éƒ¨æ•°æ®çš„æ·±åº¦é‡åŒ–æ´å¯Ÿ", "chapters": []}
    
    chapters_prompts = [
        {"title": "æ‰§è¡Œæ‘˜è¦", "prompt": f"ä½œä¸ºAIäººæ‰æˆ˜ç•¥é¡¾é—®ï¼Œè¯·åŸºäºä»¥ä¸‹æ‰€æœ‰è¡¨æ ¼æ•°æ®ï¼Œæ’°å†™ä¸€ä»½é«˜åº¦æ¦‚æ‹¬çš„æ‰§è¡Œæ‘˜è¦ï¼Œç‚¹æ˜æˆ‘ä»¬äººæ‰æ± åœ¨æ‹›è˜æ¼æ–—ã€æ ¸å¿ƒæŠ€èƒ½å‚¨å¤‡æ–¹é¢çš„ç°çŠ¶ã€ä¼˜åŠ¿ä¸æŒ‘æˆ˜ã€‚\n\næ‹›è˜æ¼æ–—æ•°æ®:\n{tables.get('status_distribution', pd.DataFrame()).to_markdown()}\n\næŠ€èƒ½åˆ†å¸ƒæ•°æ®:\n{tables.get('skills_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "äººæ‰å¸‚åœºå®è§‚åˆ†æ", "prompt": "ä½œä¸ºè¡Œä¸šåˆ†æå¸ˆï¼Œè¯·å•çº¯åŸºäºä½ çš„çŸ¥è¯†åº“ï¼Œåˆ†æå½“å‰AIå¤§æ¨¡å‹ç®—æ³•å·¥ç¨‹å¸ˆå¸‚åœºçš„ä¾›éœ€æƒ…å†µï¼Œå¹¶è¯¦ç»†é˜è¿°â€œå¿…å¤‡æŠ€èƒ½â€ï¼ˆå¦‚Python, PyTorchï¼‰å’Œâ€œé«˜ä»·æŠ€èƒ½â€ï¼ˆå¦‚åˆ†å¸ƒå¼è®­ç»ƒ, RLHF, CUDAä¼˜åŒ–ï¼‰ä¹‹é—´çš„åŒºåˆ«å’Œä»·å€¼ã€‚"},
        {"title": "æ‹›è˜æ¼æ–—ä¸è½¬åŒ–åˆ†æ", "prompt": f"è¯·è§£è¯»ä¸‹é¢çš„â€œæ‹›è˜å„é˜¶æ®µå€™é€‰äººæ•°é‡åˆ†å¸ƒâ€è¡¨æ ¼ã€‚åˆ†ææˆ‘ä»¬çš„æ‹›è˜æ¼æ–—å½¢æ€æ˜¯å¦å¥åº·ï¼Ÿåœ¨å“ªä¸ªç¯èŠ‚å¯èƒ½å­˜åœ¨ç“¶é¢ˆæˆ–æµå¤±ç‡è¿‡é«˜çš„é—®é¢˜ï¼Ÿå¹¶æå‡ºåˆæ­¥çš„è§‚å¯Ÿã€‚\n\n{tables.get('status_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "æ ¸å¿ƒæŠ€èƒ½å›¾è°±é‡åŒ–åˆ†æ", "prompt": f"è¯·è§£è¯»ä¸‹é¢çš„â€œç»¼åˆé«˜é¢‘æŠ€èƒ½è¯ç»Ÿè®¡â€è¡¨æ ¼ã€‚åŸºäºè¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å½“å‰äººæ‰æ± çš„æ•´ä½“æŠ€æœ¯ç”»åƒæ˜¯æ€æ ·çš„ï¼Ÿå“ªäº›æŠ€æœ¯æ–¹å‘çš„äººæ‰å‚¨å¤‡ç›¸å¯¹å……è¶³ï¼Œå“ªäº›å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Ÿ\n\n{tables.get('skills_distribution', pd.DataFrame()).to_markdown()}"},
        {"title": "å¿…å¤‡æŠ€èƒ½ vs. é«˜ä»·æŠ€èƒ½è¯„ä¼°", "prompt": f"è¿™å¼ è¡¨æ ¼å¯¹æ¯”äº†æˆ‘ä»¬å®šä¹‰çš„ä¸¤ç±»æŠ€èƒ½çš„è¦†ç›–æƒ…å†µã€‚è¯·æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬çš„å€™é€‰äººæ± åœ¨æ»¡è¶³â€œå¿…å¤‡æŠ€èƒ½â€åŸºç¡€è¦æ±‚æ–¹é¢åšå¾—å¦‚ä½•ï¼Ÿåœ¨å¸å¼•å…·å¤‡â€œé«˜ä»·æŠ€èƒ½â€çš„é¡¶å°–äººæ‰æ–¹é¢åˆè¡¨ç°å¦‚ä½•ï¼Ÿè¿™æ­ç¤ºäº†æˆ‘ä»¬æ‹›è˜ç­–ç•¥çš„å“ªäº›é—®é¢˜ï¼Ÿ\n\n{tables.get('must_have_vs_high_value', pd.DataFrame()).to_markdown()}"},
        {"title": "æˆ˜ç•¥è¡ŒåŠ¨å»ºè®®", "prompt": f"ç»¼åˆä»¥ä¸Šæ‰€æœ‰è¡¨æ ¼å’Œä½ çš„è§£è¯»ï¼Œè¯·ä¸ºæˆ‘ä»¬çš„æ‹›è˜å›¢é˜Ÿæå‡º3-5æ¡å…·ä½“çš„ã€å¯ç«‹å³æ‰§è¡Œçš„æˆ˜ç•¥å»ºè®®ï¼Œä»¥ä¼˜åŒ–æ‹›è˜æ¼æ–—ã€å¼¥è¡¥æŠ€èƒ½çŸ­æ¿ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°å¸å¼•é«˜ä»·å€¼äººæ‰ã€‚"}
    ]
    
    for chapter in chapters_prompts:
        print(f"  - æ­£åœ¨ç”Ÿæˆç« èŠ‚: [{chapter['title']}]...")
        content = ask_gemini(gemini_model, chapter['prompt'])
        narratives['chapters'].append({"title": chapter['title'], "content": content})
        
    print("âœ… æ‰€æœ‰æŠ¥å‘Šç« èŠ‚å·²ç”±AIç”Ÿæˆï¼")
    return narratives, tables

# --- 5. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    setup_environment()
    notion, gemini_model, config = initialize()
    
    df_notion_data = fetch_notion_data(notion, config["CANDIDATE_DB_ID"])
    resumes_text = read_resumes_from_folder('resumes')

    if df_notion_data.empty and not resumes_text:
        print("\nâŒ è‡´å‘½é”™è¯¯: æ— æ•°æ®å¯ä¾›åˆ†æã€‚")
    else:
        try:
            with open('jd_template.txt', 'r', encoding='utf-8') as f:
                jd_text = f.read()
        except FileNotFoundError:
            print("âŒ é”™è¯¯: 'jd_template.txt' æ–‡ä»¶æœªæ‰¾åˆ°ã€‚"); sys.exit(1)
            
        narratives, tables = process_data_and_generate_narratives(gemini_model, jd_text, df_notion_data, resumes_text)
        
        if narratives:
            base_filename = f"output/Talent_Analysis_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            docx_filename = f"{base_filename}.docx"
            generate_word_report(narratives, tables, docx_filename)
            
            print(f"\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼BIç‰ˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
            print(f"   - Wordç‰ˆ (åŒ…å«æ–‡å­—ä¸è¡¨æ ¼): {docx_filename}")

    print("\n" + "="*70); print("æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•...")